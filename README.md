> æœ¬äººè¿›å…¥å¤§æ•°æ®é¢†åŸŸä»…ä»…ä¸€å¹´ï¼Œä¹‹å‰ä»æœªæ¥è§¦è¿‡å¤§æ•°æ®é¢†åŸŸï¼Œå¯¹äºSparkæ›´æ˜¯é—»æ‰€æœªé—»ï¼Œç›®å‰å…¬å¸ä¸»è¦åšIOTé¢†åŸŸã€‚å¤§æ•°æ®æŠ€æœ¯æ˜¯å¿…é¡»è¦æŒæ¡çš„ï¼Œ
> è€Œä¸”é¡¹ç›®ä¸­ä¹Ÿä½¿ç”¨åˆ°äº†Sparkï¼Œæ‰€ä»¥å­¦ä¹ ä¸‹ï¼ŒSparkæ˜¯ä½¿ç”¨Scalaå¼€å‘çš„ã€‚æœ€å¥½ä¹ŸæŒæ¡ä¸‹Scalaè¯­è¨€ã€‚
> æˆ‘å­¦ä¹ Sparkä¸»è¦æœ‰é€šè¿‡å®˜ç½‘ã€å°šç¡…è°·(ä¸å¾—ä¸è¯´å°šç¡…è°·çœŸçš„æ˜¯ä¸šç•Œè‰¯å¿ƒï¼Œä¸æ˜¯æ‰“å¹¿å‘Šå•ŠğŸ˜„)ï¼Œå¦å¤–å°±æ˜¯ä¸æ˜ç™½çš„æ—¶å€™é—®é—®åº¦å¨˜ã€‚
> æœ¬ç¯‡æ–‡ç« æ˜¯æˆ‘åœ¨å­¦ä¹ äº†ä¸€éä¹‹åé‡æ–°æ¥å†™çš„ï¼Œä¸€æ˜¯ä¸ºäº†åŠ æ·±è®°å¿†ï¼ŒäºŒæ˜¯æƒ³åˆ†äº«å‡ºæ¥ï¼Œè·Ÿç½‘å‹äº’ç›¸å­¦ä¹ ï¼Œå–é•¿è¡¥çŸ­ï¼Œå…±åŒè¿›æ­¥ã€‚

# ä»€ä¹ˆæ˜¯Spark?
![](https://itlab1024-1256529903.cos.ap-beijing.myqcloud.com/202208312008157.png)
Spark æ˜¯ApacheåŸºé‡‘ä¼šå¼€å‘çš„ç”¨äºå¤§æ•°æ®å¤„ç†çš„ç»Ÿä¸€åˆ†æå¼•æ“ã€‚å®ƒæä¾› Javaã€Scalaã€Python å’Œ R è¯­è¨€ä¸­çš„é«˜çº§ APIï¼Œ
è¿˜æœ‰æœºå™¨å­¦ä¹ ã€å›¾å½¢å¤„ç†ç­‰ï¼ˆè¿™ä¸¤ä¸ªæˆ‘ç›®å‰ç”¨ä¸åˆ°ï¼Œå…ˆä¸å­¦äº†ã€‚ï¼‰

# ä¸‹è½½å®‰è£…
ç›®å‰æœ€æ–°ç‰ˆæœ¬æ˜¯3.3.0ã€‚æˆ‘å°±æ˜¯ç”¨è¿™ä¸ªç‰ˆæœ¬è¿›è¡Œå­¦ä¹ äº†ã€‚
æ‰“å¼€[Sparkä¸‹è½½åœ°å€](https://spark.apache.org/downloads.html)
![](https://itlab1024-1256529903.cos.ap-beijing.myqcloud.com/202208312006177.png)
ä¸‹è½½å®Œæˆåï¼Œè§£å‹ï¼Œå¦‚ä¸‹å›¾ï¼š
![](https://itlab1024-1256529903.cos.ap-beijing.myqcloud.com/202208312037740.png)
# äº¤äº’Shellå’Œè¿è¡Œç¤ºä¾‹
sparkçš„binç›®å½•ä¸‹æä¾›äº†ä¸€ä¸ªäº¤äº’shellï¼Œå¹¶ä¸”ä»–æä¾›äº†å¾ˆå¤šä¾‹å­ï¼Œæ¥ä¸‹æ¥æˆ‘å°±è¿è¡Œspark-shellè„šæœ¬ï¼Œæ‰“å¼€äº¤äº’å‘½ä»¤è¡Œ
```shell
âœ  spark-3.3.0-bin-hadoop3 bin/spark-shell 
22/08/31 20:12:00 WARN Utils: Your hostname, ITshiyanshideMacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 10.112.82.59 instead (on interface en0)
22/08/31 20:12:00 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
22/08/31 20:12:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Spark context Web UI available at http://10.112.82.59:4040
Spark context available as 'sc' (master = local[*], app id = local-1661947928082).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.3.0
      /_/
         
Using Scala version 2.12.15 (Java HotSpot(TM) 64-Bit Server VM, Java 17.0.3.1)
Type in expressions to have them evaluated.
Type :help for more information.

scala> 
```
å¯ä»¥çœ‹åˆ°ä¸Šé¢å°±æ‰“å¼€äº†ä¸€ä¸ªshellã€‚åœ¨è¿™é‡Œå°±å¯ä»¥è¾“å‡ºscalaä»£ç ï¼Œæ¯”å¦‚ï¼š
```scala
scala> val i = 1
i: Int = 1

scala> val j = 1
j: Int = 1
scala> println(i + j)
2
```
æ²¡æœ‰é—®é¢˜ã€‚sparkä¸­å·²ç»ä¸ºæˆ‘ä»¬åˆå§‹åŒ–äº†sparkContextå’ŒsparkSessionï¼Œå˜é‡åˆ†åˆ«å¯¹åº”çš„æ˜¯scå’Œsparkã€‚
ä¹Ÿå¯ä»¥æŒ‡å®šå‚æ•°å¯åŠ¨shellï¼Œæ¯”å¦‚
```shell
âœ  spark-3.3.0-bin-hadoop3 bin/spark-shell --master 'local[2]' 
22/08/31 20:21:06 WARN Utils: Your hostname, ITshiyanshideMacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 10.112.82.59 instead (on interface en0)
22/08/31 20:21:06 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
22/08/31 20:21:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Spark context Web UI available at http://10.112.82.59:4040
Spark context available as 'sc' (master = local[2], app id = local-1661948473413).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.3.0
      /_/
         
Using Scala version 2.12.15 (Java HotSpot(TM) 64-Bit Server VM, Java 17.0.3.1)
Type in expressions to have them evaluated.
Type :help for more information.

scala> 
```
localä»£è¡¨æœ¬åœ°æ¨¡å¼ï¼Œsparkè¿˜æœ‰standaloneæ¨¡å¼ï¼Œyarnæ¨¡å¼ç­‰ç­‰ï¼Œæ…¢æ…¢å­¦ã€‚[2]è¿™ä¸ª2ä»£è¡¨çº¿ç¨‹æ•°ï¼Œå…·ä½“å«ä¹‰ä»¥åæ…¢æ…¢å­¦ä¹ ã€‚

æ¥ä¸‹æ¥æˆ‘ä½¿ç”¨run-exampleæ¥è¿è¡Œä¸€ä¸ªå†…ç½®çš„PIè®¡ç®—çš„ä»»åŠ¡ã€‚
```shell
âœ  spark-3.3.0-bin-hadoop3 bin/run-example SparkPi 10
22/08/31 20:18:25 WARN Utils: Your hostname, ITshiyanshideMacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 10.112.82.59 instead (on interface en0)
22/08/31 20:18:25 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
22/08/31 20:18:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
22/08/31 20:18:25 INFO SparkContext: Running Spark version 3.3.0
22/08/31 20:18:25 INFO ResourceUtils: ==============================================================
22/08/31 20:18:25 INFO ResourceUtils: No custom resources configured for spark.driver.
22/08/31 20:18:25 INFO ResourceUtils: ==============================================================
22/08/31 20:18:25 INFO SparkContext: Submitted application: Spark Pi
22/08/31 20:18:25 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
22/08/31 20:18:26 INFO ResourceProfile: Limiting resource is cpu
22/08/31 20:18:26 INFO ResourceProfileManager: Added ResourceProfile id: 0
22/08/31 20:18:26 INFO SecurityManager: Changing view acls to: itlab
22/08/31 20:18:26 INFO SecurityManager: Changing modify acls to: itlab
22/08/31 20:18:26 INFO SecurityManager: Changing view acls groups to: 
22/08/31 20:18:26 INFO SecurityManager: Changing modify acls groups to: 
22/08/31 20:18:26 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(itlab); groups with view permissions: Set(); users  with modify permissions: Set(itlab); groups with modify permissions: Set()
22/08/31 20:18:26 INFO Utils: Successfully started service 'sparkDriver' on port 65391.
22/08/31 20:18:26 INFO SparkEnv: Registering MapOutputTracker
22/08/31 20:18:26 INFO SparkEnv: Registering BlockManagerMaster
22/08/31 20:18:26 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
22/08/31 20:18:26 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
22/08/31 20:18:26 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
22/08/31 20:18:26 INFO DiskBlockManager: Created local directory at /private/var/folders/b7/yhnw9hws0ng2w1_khl8nr2t40000gn/T/blockmgr-006f1115-02e9-4883-96c4-7deb4c9c0f2c
22/08/31 20:18:26 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
22/08/31 20:18:26 INFO SparkEnv: Registering OutputCommitCoordinator
22/08/31 20:18:27 INFO Utils: Successfully started service 'SparkUI' on port 4040.
22/08/31 20:18:27 INFO SparkContext: Added JAR file:///Users/itlab/dev-tools/spark-3.3.0-bin-hadoop3/examples/jars/scopt_2.12-3.7.1.jar at spark://10.112.82.59:65391/jars/scopt_2.12-3.7.1.jar with timestamp 1661948305908
22/08/31 20:18:27 INFO SparkContext: Added JAR file:///Users/itlab/dev-tools/spark-3.3.0-bin-hadoop3/examples/jars/spark-examples_2.12-3.3.0.jar at spark://10.112.82.59:65391/jars/spark-examples_2.12-3.3.0.jar with timestamp 1661948305908
22/08/31 20:18:27 INFO SparkContext: The JAR file:/Users/itlab/dev-tools/spark-3.3.0-bin-hadoop3/examples/jars/spark-examples_2.12-3.3.0.jar at spark://10.112.82.59:65391/jars/spark-examples_2.12-3.3.0.jar has been added already. Overwriting of added jar is not supported in the current version.
22/08/31 20:18:27 INFO Executor: Starting executor ID driver on host 10.112.82.59
22/08/31 20:18:27 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
22/08/31 20:18:27 INFO Executor: Fetching spark://10.112.82.59:65391/jars/scopt_2.12-3.7.1.jar with timestamp 1661948305908
22/08/31 20:18:27 INFO TransportClientFactory: Successfully created connection to /10.112.82.59:65391 after 39 ms (0 ms spent in bootstraps)
22/08/31 20:18:27 INFO Utils: Fetching spark://10.112.82.59:65391/jars/scopt_2.12-3.7.1.jar to /private/var/folders/b7/yhnw9hws0ng2w1_khl8nr2t40000gn/T/spark-12cd6bb6-84c2-4145-90bc-0aa083c2b6c4/userFiles-ce4cfeb0-70e4-42b7-9f37-e4143760ff2a/fetchFileTemp5878280159627562337.tmp
22/08/31 20:18:27 INFO Executor: Adding file:/private/var/folders/b7/yhnw9hws0ng2w1_khl8nr2t40000gn/T/spark-12cd6bb6-84c2-4145-90bc-0aa083c2b6c4/userFiles-ce4cfeb0-70e4-42b7-9f37-e4143760ff2a/scopt_2.12-3.7.1.jar to class loader
22/08/31 20:18:27 INFO Executor: Fetching spark://10.112.82.59:65391/jars/spark-examples_2.12-3.3.0.jar with timestamp 1661948305908
22/08/31 20:18:27 INFO Utils: Fetching spark://10.112.82.59:65391/jars/spark-examples_2.12-3.3.0.jar to /private/var/folders/b7/yhnw9hws0ng2w1_khl8nr2t40000gn/T/spark-12cd6bb6-84c2-4145-90bc-0aa083c2b6c4/userFiles-ce4cfeb0-70e4-42b7-9f37-e4143760ff2a/fetchFileTemp2553748991312385625.tmp
22/08/31 20:18:27 INFO Executor: Adding file:/private/var/folders/b7/yhnw9hws0ng2w1_khl8nr2t40000gn/T/spark-12cd6bb6-84c2-4145-90bc-0aa083c2b6c4/userFiles-ce4cfeb0-70e4-42b7-9f37-e4143760ff2a/spark-examples_2.12-3.3.0.jar to class loader
22/08/31 20:18:27 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 65394.
22/08/31 20:18:27 INFO NettyBlockTransferService: Server created on 10.112.82.59:65394
22/08/31 20:18:27 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
22/08/31 20:18:27 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.112.82.59, 65394, None)
22/08/31 20:18:27 INFO BlockManagerMasterEndpoint: Registering block manager 10.112.82.59:65394 with 434.4 MiB RAM, BlockManagerId(driver, 10.112.82.59, 65394, None)
22/08/31 20:18:27 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.112.82.59, 65394, None)
22/08/31 20:18:27 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.112.82.59, 65394, None)
22/08/31 20:18:28 INFO SparkContext: Starting job: reduce at SparkPi.scala:38
22/08/31 20:18:28 INFO DAGScheduler: Got job 0 (reduce at SparkPi.scala:38) with 10 output partitions
22/08/31 20:18:28 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkPi.scala:38)
22/08/31 20:18:28 INFO DAGScheduler: Parents of final stage: List()
22/08/31 20:18:28 INFO DAGScheduler: Missing parents: List()
22/08/31 20:18:28 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at map at SparkPi.scala:34), which has no missing parents
22/08/31 20:18:28 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 4.0 KiB, free 434.4 MiB)
22/08/31 20:18:28 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 2.3 KiB, free 434.4 MiB)
22/08/31 20:18:28 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.112.82.59:65394 (size: 2.3 KiB, free: 434.4 MiB)
22/08/31 20:18:28 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1513
22/08/31 20:18:28 INFO DAGScheduler: Submitting 10 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at SparkPi.scala:34) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))
22/08/31 20:18:28 INFO TaskSchedulerImpl: Adding task set 0.0 with 10 tasks resource profile 0
22/08/31 20:18:28 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.112.82.59, executor driver, partition 0, PROCESS_LOCAL, 4578 bytes) taskResourceAssignments Map()
22/08/31 20:18:28 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (10.112.82.59, executor driver, partition 1, PROCESS_LOCAL, 4578 bytes) taskResourceAssignments Map()
22/08/31 20:18:28 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2) (10.112.82.59, executor driver, partition 2, PROCESS_LOCAL, 4578 bytes) taskResourceAssignments Map()
22/08/31 20:18:28 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3) (10.112.82.59, executor driver, partition 3, PROCESS_LOCAL, 4578 bytes) taskResourceAssignments Map()
22/08/31 20:18:28 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
22/08/31 20:18:28 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
22/08/31 20:18:28 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
22/08/31 20:18:28 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
22/08/31 20:18:28 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 1008 bytes result sent to driver
22/08/31 20:18:28 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 1008 bytes result sent to driver
22/08/31 20:18:28 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1008 bytes result sent to driver
22/08/31 20:18:28 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1008 bytes result sent to driver
22/08/31 20:18:28 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4) (10.112.82.59, executor driver, partition 4, PROCESS_LOCAL, 4578 bytes) taskResourceAssignments Map()
22/08/31 20:18:28 INFO Executor: Running task 4.0 in stage 0.0 (TID 4)
22/08/31 20:18:28 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5) (10.112.82.59, executor driver, partition 5, PROCESS_LOCAL, 4578 bytes) taskResourceAssignments Map()
22/08/31 20:18:28 INFO Executor: Running task 5.0 in stage 0.0 (TID 5)
22/08/31 20:18:28 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6) (10.112.82.59, executor driver, partition 6, PROCESS_LOCAL, 4578 bytes) taskResourceAssignments Map()
22/08/31 20:18:28 INFO Executor: Running task 6.0 in stage 0.0 (TID 6)
22/08/31 20:18:28 INFO TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7) (10.112.82.59, executor driver, partition 7, PROCESS_LOCAL, 4578 bytes) taskResourceAssignments Map()
22/08/31 20:18:28 INFO Executor: Running task 7.0 in stage 0.0 (TID 7)
22/08/31 20:18:28 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 334 ms on 10.112.82.59 (executor driver) (1/10)
22/08/31 20:18:28 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 323 ms on 10.112.82.59 (executor driver) (2/10)
22/08/31 20:18:28 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 332 ms on 10.112.82.59 (executor driver) (3/10)
22/08/31 20:18:28 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 338 ms on 10.112.82.59 (executor driver) (4/10)
22/08/31 20:18:28 INFO Executor: Finished task 5.0 in stage 0.0 (TID 5). 965 bytes result sent to driver
22/08/31 20:18:28 INFO TaskSetManager: Starting task 8.0 in stage 0.0 (TID 8) (10.112.82.59, executor driver, partition 8, PROCESS_LOCAL, 4578 bytes) taskResourceAssignments Map()
22/08/31 20:18:28 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 139 ms on 10.112.82.59 (executor driver) (5/10)
22/08/31 20:18:28 INFO Executor: Running task 8.0 in stage 0.0 (TID 8)
22/08/31 20:18:29 INFO Executor: Finished task 4.0 in stage 0.0 (TID 4). 965 bytes result sent to driver
22/08/31 20:18:29 INFO TaskSetManager: Starting task 9.0 in stage 0.0 (TID 9) (10.112.82.59, executor driver, partition 9, PROCESS_LOCAL, 4578 bytes) taskResourceAssignments Map()
22/08/31 20:18:29 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 184 ms on 10.112.82.59 (executor driver) (6/10)
22/08/31 20:18:29 INFO Executor: Finished task 7.0 in stage 0.0 (TID 7). 965 bytes result sent to driver
22/08/31 20:18:29 INFO Executor: Running task 9.0 in stage 0.0 (TID 9)
22/08/31 20:18:29 INFO TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 180 ms on 10.112.82.59 (executor driver) (7/10)
22/08/31 20:18:29 INFO Executor: Finished task 6.0 in stage 0.0 (TID 6). 965 bytes result sent to driver
22/08/31 20:18:29 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 190 ms on 10.112.82.59 (executor driver) (8/10)
22/08/31 20:18:29 INFO Executor: Finished task 8.0 in stage 0.0 (TID 8). 965 bytes result sent to driver
22/08/31 20:18:29 INFO TaskSetManager: Finished task 8.0 in stage 0.0 (TID 8) in 77 ms on 10.112.82.59 (executor driver) (9/10)
22/08/31 20:18:29 INFO Executor: Finished task 9.0 in stage 0.0 (TID 9). 965 bytes result sent to driver
22/08/31 20:18:29 INFO TaskSetManager: Finished task 9.0 in stage 0.0 (TID 9) in 69 ms on 10.112.82.59 (executor driver) (10/10)
22/08/31 20:18:29 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
22/08/31 20:18:29 INFO DAGScheduler: ResultStage 0 (reduce at SparkPi.scala:38) finished in 0.808 s
22/08/31 20:18:29 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
22/08/31 20:18:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
22/08/31 20:18:29 INFO DAGScheduler: Job 0 finished: reduce at SparkPi.scala:38, took 0.882169 s
Pi is roughly 3.14032714032714
22/08/31 20:18:29 INFO SparkUI: Stopped Spark web UI at http://10.112.82.59:4040
22/08/31 20:18:29 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
22/08/31 20:18:29 INFO MemoryStore: MemoryStore cleared
22/08/31 20:18:29 INFO BlockManager: BlockManager stopped
22/08/31 20:18:29 INFO BlockManagerMaster: BlockManagerMaster stopped
22/08/31 20:18:29 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
22/08/31 20:18:29 INFO SparkContext: Successfully stopped SparkContext
22/08/31 20:18:29 INFO ShutdownHookManager: Shutdown hook called
22/08/31 20:18:29 INFO ShutdownHookManager: Deleting directory /private/var/folders/b7/yhnw9hws0ng2w1_khl8nr2t40000gn/T/spark-12cd6bb6-84c2-4145-90bc-0aa083c2b6c4
22/08/31 20:18:29 INFO ShutdownHookManager: Deleting directory /private/var/folders/b7/yhnw9hws0ng2w1_khl8nr2t40000gn/T/spark-250b1c99-4798-4faf-b534-6ecdac448570
```
å¯ä»¥çœ‹åˆ°æ—¥å¿—ä¸­ï¼šPi is roughly 3.14032714032714ï¼Œè¿™å°±æ˜¯Piçš„è¿ç®—ç»“æœã€‚
# å¿«é€Ÿä¸Šæ‰‹
Sparkå¼€å‘å¯ä»¥ä½¿ç”¨VS Codeå·¥å…·æˆ–è€…IDEAã€‚æˆ‘ä½¿ç”¨çš„æ˜¯IDEAã€‚è®°ä¸‹æ¥ç®€å•ä»‹ç»ä¸‹ä½¿ç”¨IDEAåˆ›å»ºä¸€ä¸ªé¡¹ç›®ã€‚
å› ä¸ºæˆ‘è¦ä½¿ç”¨scalaè¯­è¨€å»å†™sparkï¼ˆä¹Ÿå¯ä»¥ä½¿ç”¨Javaã€Pythonã€Rç­‰è¯­è¨€ï¼‰ï¼Œé¡¹ç›®ç®¡ç†å¯ä»¥ä½¿ç”¨Mavenã€Gradleã€SBT
SBTæ˜¯scalaé¡¹ç›®çš„åŒ…ç®¡ç†å·¥å…·ï¼Œæˆ‘è¿™å°±ä½¿ç”¨SBTã€‚
## å®‰è£…æ’ä»¶
IDEAå¼€å‘scalaéœ€è¦å®‰è£…scalaæ’ä»¶
![](https://itlab1024-1256529903.cos.ap-beijing.myqcloud.com/202209011344345.png)
ä½¿ç”¨ä½¿ç”¨mavenï¼Œéœ€è¦ç‚¹å‡»Fileèœå•å¯¹é¡¹ç›®è¿›è¡Œscalaæ¡†æ¶æ”¯æŒã€‚
## Hello World
æ­å»ºå®Œé¡¹ç›®åï¼Œå®ç°ä¸ªHello worldç¤ºä¾‹
```scala
package com.itlab1024.spark.start

object HelloWorld {
  def main(args: Array[String]): Unit = {
    println("hello world!") // hello world!
  }
}
```

## åˆ›å»ºé¡¹ç›®
![](https://itlab1024-1256529903.cos.ap-beijing.myqcloud.com/202209011456148.png)
## å¼•å…¥sparkä¾èµ–
è¿™é‡Œæˆ‘å…ˆæ˜ å…¥spring-coreåŒ…ã€‚å…¶ä»–çš„åŒ…ä»¥åç”¨åˆ°å†å¼•å…¥ï¼Œbuild.sbtæ˜¯sbtçš„ä¾èµ–é…ç½®ï¼Œç±»ä¼¼mavençš„pomã€‚
```sbt
ThisBuild / version := "0.1.0-SNAPSHOT"

ThisBuild / scalaVersion := "2.12.16"

lazy val root = (project in file("."))
  .settings(
    name := "Spark-Tutorial"
  )
libraryDependencies += "org.apache.spark" % "spark-core_2.12" % "3.3.0"
```
ä¸Šé¢çš„ä¾‹å­è¿˜æ²¡æœ‰ä½¿ç”¨åˆ°sparkï¼Œæ¥ä¸‹æ¥å†™ä¸€ä¸ªSparkçš„ç»å…¸ä¾‹å­ï¼ŒWordCountã€‚ç»Ÿè®¡æ–‡æœ¬ä¸­å•è¯æ•°ç›®ã€‚
å¦‚ä½•ç»Ÿè®¡å‘¢ï¼Ÿ 
a.å°†æ–‡ä»¶ä¸­çš„æ•°æ®è¯»å…¥åˆ°å†…å­˜ï¼Œç»“æœæ˜¯ä¸€è¡Œä¸€è¡Œçš„ã€‚ 
b.å°†æ¯è¡Œé€šè¿‡ç©ºæ ¼åˆ‡åˆ† 
c.è½¬åŒ–ä¸ºå…ƒç»„ï¼Œæ¯”å¦‚(K,V)ï¼ŒKä»£è¡¨å•è¯ï¼ŒVä»£è¡¨å•è¯çš„æ•°é‡ï¼ˆå†™æ­»1ï¼‰
d.ç„¶åé€šè¿‡Kèšåˆå°†æ‰€æœ‰VåŠ èµ·
æ–°å»ºä¸€ä¸ªwordCount.txt
```text
I am learning spark
I am learning go
I am learning scala
I am learning java
```
æ¥ä¸‹æ¥é€šè¿‡ä»£ç æ¥å®ç°è¯¥åŠŸèƒ½
```scala
package com.itlab1024.spark.start

import org.apache.spark.rdd.RDD
import org.apache.spark.{SparkConf, SparkContext}

object WordCount {
  def main(args: Array[String]): Unit = {
    // å®šä¹‰é…ç½®ï¼Œé€šè¿‡é…ç½®å»ºç«‹è¿æ¥
    val conf: SparkConf = new SparkConf().setAppName("ç»Ÿè®¡å•è¯æ•°é‡").setMaster("local[*]")
    val sc: SparkContext = new SparkContext(conf)
    //1. å°†æ–‡ä»¶ä¸­çš„æ•°æ®è¯»å…¥åˆ°å†…å­˜ï¼Œç»“æœæ˜¯ä¸€è¡Œä¸€è¡Œçš„ã€‚
    val rdd: RDD[String] = sc.textFile("files/wordCount.txt")
    //2. å°†æ¯è¡Œé€šè¿‡ç©ºæ ¼åˆ‡åˆ†
    val flatRDD: RDD[String] = rdd.flatMap(_.split(" "))
    //3. è½¬åŒ–ä¸ºå…ƒç»„ï¼Œæ¯”å¦‚(K,V)ï¼ŒKä»£è¡¨å•è¯ï¼ŒVä»£è¡¨å•è¯çš„æ•°é‡ï¼ˆå†™æ­»1ï¼‰
    val tupleRDD: RDD[(String, Int)] = flatRDD.map((_, 1))
    //4. ç„¶åé€šè¿‡Kèšåˆå°†æ‰€æœ‰VåŠ èµ·
    val result: RDD[(String, Int)] = tupleRDD.reduceByKey(_ + _)
    // æ‰“å°
    result.collect().foreach(println)
    // å…³é—­è¿æ¥
    sc.stop()
  }
}
```
è¿è¡Œç»“æœï¼š
```text
-------çœç•¥ä¸€éƒ¨åˆ†æ—¥å¿—------
22/09/01 15:13:46 INFO DAGScheduler: Job 0 finished: collect at WordCount.scala:20, took 3.147017 s
(scala,1)
(learning,4)
(spark,1)
(am,4)
(I,4)
(java,1)
(go,1)
22/09/01 15:13:46 INFO SparkUI: Stopped Spark web UI at http://10.112.82.59:4040
-------çœç•¥ä¸€éƒ¨åˆ†æ—¥å¿—------
```
å¯ä»¥çœ‹åˆ°æ—¥å¿—ä¸­æ‰“å°å‡ºæ¥äº†ç»Ÿè®¡çš„ç»“æœï¼š
```text
(scala,1)
(learning,4)
(spark,1)
(am,4)
(I,4)
(java,1)
(go,1)
```

> çœ‹åˆ°ä¸Šé¢çš„ä»£ç æœ‰å¾ˆå¤šä¸ç†è§£çš„åœ°æ–¹ï¼Œæ¯”å¦‚setMasteré‡Œçš„localæ˜¯ä»€ä¹ˆæ„æ€ï¼ŸRDDæ˜¯ä»€ä¹ˆï¼Ÿ
> åˆ«ç€æ€¥æ¥ä¸‹æ¥æ…¢æ…¢å­¦ä¹ ã€‚

# è¿è¡Œç¯å¢ƒ
Sparkçš„è¿è¡Œç¯å¢ƒæœ‰å¼€å‘ç¯å¢ƒã€æœ¬åœ°ç¯å¢ƒã€ç‹¬ç«‹ç¯å¢ƒï¼ˆStandaloneï¼‰ã€Hadoop Yarnæ¨¡å¼ã€Kubernetesç¯å¢ƒã€‚ ##å¼€å‘æ¨¡å¼ï¼šä¸Šé¢æˆ‘ä»¬æ‰§è¡ŒWordCountä»£ç çš„ç¯å¢ƒå°±æ˜¯å¼€å‘ç¯å¢ƒï¼Œä¸¥æ ¼æ¥è¯´ä»–å¹¶ä¸æ˜¯ä¸€ç§ç¯å¢ƒï¼Œä»…ä»…ç”¨äºå¼€å‘ã€‚
* **æœ¬åœ°æ¨¡å¼**ï¼šä½¿ç”¨spark-shellå¼€å¯çš„ç¯å¢ƒå°±æ˜¯æœ¬åœ°ç¯å¢ƒï¼Œç”¨äºå¼€å‘ã€æµ‹è¯•ã€è°ƒè¯•ã€æ¼”ç¤ºç­‰åŸºæœ¬ä½¿ç”¨ã€‚
* **ç‹¬ç«‹æ¨¡å¼Stanalone**ï¼šç‹¬ç«‹æ¨¡å¼æ˜¯æœ€ç®€å•çš„æ¨¡å¼ï¼Œä»–æ˜¯ä¸»ä»æ¶æ„ï¼Œç”Ÿäº§å¯ç”¨ã€‚
* **Hadoop Yarnæ¨¡å¼**ï¼šæ®è¯´å›½å†…ä¸»æµï¼Œå’±ä¹Ÿä¸æ¸…æ¥šï¼Œç”Ÿäº§å¯ç”¨ã€‚
* **Kubernetesæ¨¡å¼**ï¼šè¿™ä¸ªæˆ‘è§‰å¾—è‚¯å®šæ˜¯æµè¡Œçš„ï¼Œå› ä¸ºå®¹å™¨åŒ–ç°åœ¨éå¸¸æµè¡Œï¼Œç”Ÿäº§å¯ç”¨ã€‚
![](https://itlab1024-1256529903.cos.ap-beijing.myqcloud.com/202209011528643.png)

## å¼€å‘ç¯å¢ƒ
æ²¡å•¥å¥½è¯´çš„
## æœ¬åœ°æ¨¡å¼(å•æœº)
å…¶å®ä¹‹å‰è®²è§£shellçš„æ—¶å€™å·²ç»ä½¿ç”¨äº†æœ¬åœ°æ¨¡å¼ï¼Œè¿™é‡Œä¸»è¦è¯´ä¸‹ä¹‹å‰æ²¡æœ‰ä»‹ç»çš„
WebUI,æœ¬åœ°ç¯å¢ƒä¼šå¯åŠ¨ä¸€ä¸ªWebUIç•Œé¢,å¯åŠ¨æ—¥å¿—ä¸­æˆ‘ä»¬å¯ä»¥çœ‹åˆ°å¦‚ä¸‹æ—¥å¿—ï¼š
```text
22/09/01 15:39:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Spark context Web UI available at http://10.112.82.59:4040
Spark context available as 'sc' (master = local[*], app id = local-1662017962631).
```
http://10.112.82.59:4040å°±æ˜¯webç•Œé¢
![](https://itlab1024-1256529903.cos.ap-beijing.myqcloud.com/202209011540886.png)
æˆ‘ä»¬å°†ä¸Šé¢WordCountçš„ä»£ç æ”¾åˆ°shellä¸­æ‰§è¡Œï¼Œé¦–å…ˆè¦åœ¨sparkå®‰è£…ç›®å½•ä¸‹åˆ›å»ºwordCount.txtæ–‡ä»¶ã€‚
```shell
âœ  files pwd
/Users/itlab/dev-tools/spark-3.3.0-bin-hadoop3/files
âœ  files cat wordCount.txt 
I am learning spark
I am learning go
I am learning scala
I am learning java
```
æ¥ä¸‹æ¥åœ¨shellä¸­æ‰§è¡ŒWordCountçš„ä»£ç 
```shell
scala> sc.textFile("files/wordCount.txt")
res2: org.apache.spark.rdd.RDD[String] = files/wordCount.txt MapPartitionsRDD[5] at textFile at <console>:24

scala> val rdd = sc.textFile("files/wordCount.txt")
rdd: org.apache.spark.rdd.RDD[String] = files/wordCount.txt MapPartitionsRDD[7] at textFile at <console>:23

scala> val flatRDD = rdd.flatMap(_.split(" "))
flatRDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[8] at flatMap at <console>:23

scala> val tupleRDD = flatRDD.map((_, 1))
tupleRDD: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[9] at map at <console>:23

scala> val result = tupleRDD.reduceByKey(_ + _)
result: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[10] at reduceByKey at <console>:23

scala> result.collect().foreach(println)
(scala,1)
(learning,4)
(am,4)
(java,1)
(go,1)
(spark,1)
(I,4)
```
å¦‚æœä¸é™æ–‡ä»¶æ”¾å…¥sparkç›®å½•ä¸‹ä¼šæç¤ºå¦‚ä¸‹
```text
org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/Users/itlab/dev-tools/spark-3.3.0-bin-hadoop3/files/wordCount.txt
  at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)
  at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)
  at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)
  at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:208)
  at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:292)
  at scala.Option.getOrElse(Option.scala:189)
  at org.apache.spark.rdd.RDD.partitions(RDD.scala:288)
  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
  at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:292)
  at scala.Option.getOrElse(Option.scala:189)
  at org.apache.spark.rdd.RDD.partitions(RDD.scala:288)
  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
  at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:292)
  at scala.Option.getOrElse(Option.scala:189)
  at org.apache.spark.rdd.RDD.partitions(RDD.scala:288)
  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
  at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:292)
  at scala.Option.getOrElse(Option.scala:189)
  at org.apache.spark.rdd.RDD.partitions(RDD.scala:288)
  at org.apache.spark.Partitioner$.$anonfun$defaultPartitioner$4(Partitioner.scala:78)
  at org.apache.spark.Partitioner$.$anonfun$defaultPartitioner$4$adapted(Partitioner.scala:78)
  at scala.collection.immutable.List.map(List.scala:293)
  at org.apache.spark.Partitioner$.defaultPartitioner(Partitioner.scala:78)
  at org.apache.spark.rdd.PairRDDFunctions.$anonfun$reduceByKey$4(PairRDDFunctions.scala:323)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
  at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)
  at org.apache.spark.rdd.PairRDDFunctions.reduceByKey(PairRDDFunctions.scala:323)
  ... 47 elided
Caused by: java.io.IOException: Input path does not exist: file:/Users/itlab/dev-tools/spark-3.3.0-bin-hadoop3/files/wordCount.txt
  at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)
  ... 74 more
```
é‡æ–°æ‰“å¼€WebUIï¼Œä¼šçœ‹åˆ°å¤šäº†ä¸€ä¸ªJobã€‚
![](https://itlab1024-1256529903.cos.ap-beijing.myqcloud.com/202209011956399.png)
è¿™ä¸ªJobå°±æ˜¯åˆšæ‰æäº¤çš„WordCountã€‚
## ç‹¬ç«‹æ¨¡å¼Standaloneï¼ˆé›†ç¾¤ï¼‰
sparkç‹¬ç«‹æ¨¡å¼å®˜ç½‘æœ‰è¯¦ç»†çš„è¯´æ˜ï¼Œæ¥ä¸‹æ¥æˆ‘ç®€å•ä»‹ç»ä¸‹è¯¥æ¨¡å¼ï¼Œå¹¶åˆ›å»ºä¸€ä¸ªç‹¬ç«‹æ¨¡å¼çš„é›†ç¾¤ç¯å¢ƒã€‚
Sparkç‹¬ç«‹æ¨¡å¼é‡‡ç”¨çš„æ˜¯Master-Slaveä¸»ä»æ¨¡å¼ã€‚é›†ç¾¤ç”±è‡ªèº«ç®¡ç†ï¼Œé«˜å¯ç”¨çš„é›†ç¾¤é‡‡ç”¨ä¸»å¤‡Masterå®ç°ï¼Œé€šè¿‡Zookeeperåè°ƒã€‚
### å‡†å¤‡
é¦–å…ˆè¦å‡†å¤‡å››å°æœºå™¨ï¼ˆå¯ä»¥æ˜¯è™šæ‹Ÿæœºï¼‰ï¼Œæˆ‘ä½¿ç”¨[vagrant](https://www.vagrantup.com/)æ­é…[virtualbox](https://www.virtualbox.org/)æ¥åˆ›å»ºè™šæ‹Ÿæœº.
```shell
âœ  spark-stanalone cat Vagrantfile 
Vagrant.configure("2") do |config|
   (1..4).each do |i|
        config.vm.define "spark-standalone#{i}" do |node|
            # è®¾ç½®è™šæ‹Ÿæœºçš„Boxã€‚æŒ‡å®šæœ¬åœ°çš„boxæ–‡ä»¶
            node.vm.box = "centos/7"

            # è®¾ç½®è™šæ‹Ÿæœºçš„ä¸»æœºå
            node.vm.hostname="spark-standalone#{i}"

            # è®¾ç½®è™šæ‹Ÿæœºçš„IP
            node.vm.network "private_network", ip: "192.168.56.#{i}"

            # VirtaulBoxç›¸å…³é…ç½®
            node.vm.provider "virtualbox" do |v|
                # è®¾ç½®è™šæ‹Ÿæœºçš„åç§°
                v.name = "spark-standalone#{i}"
                # è®¾ç½®è™šæ‹Ÿæœºçš„å†…å­˜å¤§å°
                v.memory = 1024
                # è®¾ç½®è™šæ‹Ÿæœºçš„CPUä¸ªæ•°
                v.cpus = 1
            end
        end
   end
end
```
ç„¶åæ‰§è¡Œvagrant upï¼Œå¯ä»¥çœ‹åˆ°å¼€å§‹åˆ›å»ºè™šæ‹Ÿæœºäº†ï¼Œç¬¬ä¸€æ¬¡ä½¿ç”¨å¯èƒ½æ²¡æœ‰centos/7åŒ…ï¼Œä¼šè‡ªåŠ¨ä¸‹è½½ï¼Œä½†æ˜¯å› ä¸ºæœåŠ¡å™¨åœ¨å›½å¤–å¾ˆæ…¢ï¼Œå¯ä»¥å…ˆä¸‹è½½ä¸‹æ¥å†ä½¿ç”¨vagrant box addã€‚
é¢~å‡ºç°äº†å¦‚ä¸‹é—®é¢˜
```shell
âœ  spark-stanalone vagrant up
Bringing machine 'spark-standalone1' up with 'virtualbox' provider...
Bringing machine 'spark-standalone2' up with 'virtualbox' provider...
Bringing machine 'spark-standalone3' up with 'virtualbox' provider...
Bringing machine 'spark-standalone4' up with 'virtualbox' provider...
==> spark-standalone1: Box 'centos/7' could not be found. Attempting to find and install...
    spark-standalone1: Box Provider: virtualbox
    spark-standalone1: Box Version: >= 0
==> spark-standalone1: Loading metadata for box 'centos/7'
    spark-standalone1: URL: https://vagrantcloud.com/centos/7
==> spark-standalone1: Adding box 'centos/7' (v2004.01) for provider: virtualbox
    spark-standalone1: Downloading: https://vagrantcloud.com/centos/boxes/7/versions/2004.01/providers/virtualbox.box
==> spark-standalone1: Box download is resuming from prior download progress
Download redirected to host: cloud.centos.org
    spark-standalone1: Calculating and comparing box checksum...
==> spark-standalone1: Successfully added box 'centos/7' (v2004.01) for 'virtualbox'!
==> spark-standalone1: You assigned a static IP ending in ".1" to this machine.
==> spark-standalone1: This is very often used by the router and can cause the
==> spark-standalone1: network to not work properly. If the network doesn't work
==> spark-standalone1: properly, try changing this IP.
==> spark-standalone1: Importing base box 'centos/7'...
==> spark-standalone1: Matching MAC address for NAT networking...
==> spark-standalone1: You assigned a static IP ending in ".1" to this machine.
==> spark-standalone1: This is very often used by the router and can cause the
==> spark-standalone1: network to not work properly. If the network doesn't work
==> spark-standalone1: properly, try changing this IP.
==> spark-standalone1: Checking if box 'centos/7' version '2004.01' is up to date...
==> spark-standalone1: Setting the name of the VM: spark-standalone1
==> spark-standalone1: Clearing any previously set network interfaces...
There was an error while executing `VBoxManage`, a CLI used by Vagrant
for controlling VirtualBox. The command and stderr is shown below.

Command: ["hostonlyif", "create"]

Stderr: 0%...
Progress state: NS_ERROR_FAILURE
VBoxManage: error: Failed to create the host-only adapter
VBoxManage: error: VBoxNetAdpCtl: Error while adding new interface: failed to open /dev/vboxnetctl: No such file or directory
VBoxManage: error: Details: code NS_ERROR_FAILURE (0x80004005), component HostNetworkInterfaceWrap, interface IHostNetworkInterface
VBoxManage: error: Context: "RTEXITCODE handleCreate(HandlerArg *)" at line 95 of file VBoxManageHostonly.cpp
```
è§£å†³æ–¹æ¡ˆhttps://stackoverflow.com/questions/21069908/vboxmanage-error-failed-to-create-the-host-only-adapter
![](https://itlab1024-1256529903.cos.ap-beijing.myqcloud.com/202209021116771.png)
é‡æ–°æ‰§è¡Œvagrant up
```shell
âœ  spark-stanalone vagrant up
Bringing machine 'spark-standalone1' up with 'virtualbox' provider...
Bringing machine 'spark-standalone2' up with 'virtualbox' provider...
Bringing machine 'spark-standalone3' up with 'virtualbox' provider...
Bringing machine 'spark-standalone4' up with 'virtualbox' provider...
==> spark-standalone1: You assigned a static IP ending in ".1" to this machine.
==> spark-standalone1: This is very often used by the router and can cause the
==> spark-standalone1: network to not work properly. If the network doesn't work
==> spark-standalone1: properly, try changing this IP.
==> spark-standalone1: Importing base box 'centos/7'...
==> spark-standalone1: Matching MAC address for NAT networking...
==> spark-standalone1: You assigned a static IP ending in ".1" to this machine.
==> spark-standalone1: This is very often used by the router and can cause the
==> spark-standalone1: network to not work properly. If the network doesn't work
==> spark-standalone1: properly, try changing this IP.
==> spark-standalone1: Checking if box 'centos/7' version '2004.01' is up to date...
==> spark-standalone1: Setting the name of the VM: spark-standalone1
==> spark-standalone1: Clearing any previously set network interfaces...
==> spark-standalone1: Preparing network interfaces based on configuration...
    spark-standalone1: Adapter 1: nat
    spark-standalone1: Adapter 2: hostonly
==> spark-standalone1: Forwarding ports...
    spark-standalone1: 22 (guest) => 2222 (host) (adapter 1)
==> spark-standalone1: Running 'pre-boot' VM customizations...
==> spark-standalone1: Booting VM...
==> spark-standalone1: Waiting for machine to boot. This may take a few minutes...
    spark-standalone1: SSH address: 127.0.0.1:2222
    spark-standalone1: SSH username: vagrant
    spark-standalone1: SSH auth method: private key
    spark-standalone1: 
    spark-standalone1: Vagrant insecure key detected. Vagrant will automatically replace
    spark-standalone1: this with a newly generated keypair for better security.
    spark-standalone1: 
    spark-standalone1: Inserting generated public key within guest...
    spark-standalone1: Removing insecure key from the guest if it's present...
    spark-standalone1: Key inserted! Disconnecting and reconnecting using new SSH key...
==> spark-standalone1: Machine booted and ready!
==> spark-standalone1: Checking for guest additions in VM...
    spark-standalone1: No guest additions were detected on the base box for this VM! Guest
    spark-standalone1: additions are required for forwarded ports, shared folders, host only
    spark-standalone1: networking, and more. If SSH fails on this machine, please install
    spark-standalone1: the guest additions and repackage the box to continue.
    spark-standalone1: 
    spark-standalone1: This is not an error message; everything may continue to work properly,
    spark-standalone1: in which case you may ignore this message.
==> spark-standalone1: Setting hostname...
==> spark-standalone1: Configuring and enabling network interfaces...
==> spark-standalone1: Rsyncing folder: /Users/itlab/vms/spark-stanalone/ => /vagrant
.........çœç•¥å¾ˆå¤šæ—¥å¿—........
```
æ‰“å¼€VirtualBoxå¯ä»¥çœ‹åˆ°å››ä¸ªè™šæ‹Ÿæœºå·²ç»åˆ›å»ºå®Œæ¯•ã€‚
![](https://itlab1024-1256529903.cos.ap-beijing.myqcloud.com/202209021129927.png)
ä½¿ç”¨vagrant ssh ä¸»æœºåæ¥è¿æ¥è™šæ‹Ÿæœºï¼Œä¾‹å¦‚

```shell
âœ  spark-stanalone vagrant ssh spark-standalone1
[vagrant@spark-standalone1 ~]$ 
```



å››å°è™šæ‹Ÿæœºä½¿ç”¨è§’è‰²è¯´æ˜å¦‚ä¸‹ï¼š



| ä¸»æœºå            | è§’è‰²         | IPåœ°å€         |
| ----------------- | ------------ | -------------- |
| spark-standalone1 | Master       | 192.168.56.101 |
| spark-standalone2 | Master(å¤‡ç”¨) | 192.168.56.102 |
| spark-standalone3 | Slave        | 192.168.56.103 |
| spark-standalone4 | Slave        | 192.168.56.104 |

ä¿®æ”¹å››ä¸ªä¸»æœºçš„hostsï¼Œä½¿å…¶èƒ½å¤Ÿé€šè¿‡ä¸»æœºåäº’é€š

```shell
[root@spark-standalone1 vagrant]# sudo su 
[root@spark-standalone1 vagrant]# cat >> /etc/hosts << EOF
192.168.56.101 spark-standalone1
192.168.56.102 spark-standalone2
192.168.56.103 spark-standalone3
192.168.56.104 spark-standalone4
EOF
```

å››ä¸ªæœºå™¨éƒ½å®‰è£…JDKå®‰è£…ï¼Œsparkè¿è¡Œä¾èµ–JDKã€‚æˆ‘å®‰è£…çš„æ˜¯JDK1.8

```shell
sudo yum install java-1.8.0-openjdk* -y
```

### æ­å»º(æ‰‹åŠ¨)

æ‰‹åŠ¨æ­å»ºä½¿ç”¨sbinç›®å½•ä¸‹çš„start-master.shã€stop-master.shã€start-worker.shã€stop-worker.shè¿™å‡ ä¸ªæ–‡ä»¶ã€‚

ä¸ç”¨ä¿®æ”¹ä»»ä½•é…ç½®æ–‡ä»¶ï¼Œé€šè¿‡å‘½ä»¤è¡Œå‚æ•°å°±å¯ä»¥å¯åŠ¨é›†ç¾¤ã€‚

æ­¥éª¤å°±æ˜¯å…ˆå¯åŠ¨masterï¼Œç„¶åå¯åŠ¨workerï¼Œæ­¤æ—¶æŒ‡å®šmasteråœ°å€ã€‚

é¦–å…ˆå°†æœ¬åœ°ä¸‹è½½çš„sparkåŒ…ä¸Šä¼ åˆ°å››ä¸ªè™šæ‹Ÿæœºä¸­ï¼Œæˆ‘ä½¿ç”¨çš„vagrant scpæ’ä»¶ã€‚

```shell
# å®‰è£…vagrant scpæ’ä»¶
âœ  spark-stanalone vagrant plugin install vagrant-scp 
Installing the 'vagrant-scp' plugin. This can take a few minutes...
Fetching vagrant-scp-0.5.9.gem
Installed the plugin 'vagrant-scp (0.5.9)'!
# âœ  spark-stanalone vagrant scp ~/Downloads/spark-3.3.0-bin-hadoop3.tgz spark-standalone1:/home/vagrant
Warning: Permanently added '[127.0.0.1]:2222' (ED25519) to the list of known hosts.
spark-3.3.0-bin-hadoop3.tgz                   100%  285MB  42.5MB/s   00:06
# å…¶ä»–ä¸‰ä¸ªçš„æ—¥å¿—çœç•¥
```

åˆ†åˆ«åœ¨å››ä¸ªæœåŠ¡å™¨ä¸Šè§£å‹å¹¶é‡å‘½åæ–‡ä»¶å¤¹

```shell
âœ  tar zxvf spark-3.3.0-bin-hadoop3.tgz && mv spark-3.3.0-bin-hadoop3 spark-standalone
```

ç„¶åæ‰§è¡Œå¦‚ä¸‹å‘½ä»¤å¯åŠ¨Masterï¼ˆspark-standalone1èŠ‚ç‚¹ï¼‰

```shell
[vagrant@spark-standalone1 spark-standalone]$ sbin/start-master.sh -h spark-standalone1 -p 7077 --webui-port 8080
starting org.apache.spark.deploy.master.Master, logging to /home/vagrant/spark-standalone/logs/spark-vagrant-org.apache.spark.deploy.master.Master-1-spark-standalone1.out
```

å¯åŠ¨å®Œæ¯•ååœ¨å®¿ä¸»æœºä¸Šè®¿é—®materçš„webUIï¼Œhttp://spark-standalone1:8080

![](https://itlab1024-1256529903.cos.ap-beijing.myqcloud.com/202209021427746.png)

å¯ä»¥çœ‹åˆ°æ­¤æ—¶å¹¶æ²¡æœ‰workderã€‚

æ¥ä¸‹æ¥å¯åŠ¨å·¥ä½œèŠ‚ç‚¹ï¼ˆspark-standalone3å’Œspark-standalone4ï¼‰

```shell
[vagrant@spark-standalone3 spark-standalone]$ sbin/start-worker.sh spark://spark-standalone1:7077 -h spark-standalone3 --webui-port 8081
starting org.apache.spark.deploy.worker.Worker, logging to /home/vagrant/spark-standalone/logs/spark-vagrant-org.apache.spark.deploy.worker.Worker-1-spark-standalone3.out
```

spark://spark-standalone1:7077æ˜¯é›†ç¾¤åœ°å€ï¼Œåœ¨webuiä¸Šèƒ½è·å–åˆ°ã€‚

æ­¤æ—¶spark-standalone3èŠ‚ç‚¹çš„workerå·²ç»å¯åŠ¨ï¼Œçœ‹çœ‹webUIä¸Šæœ‰ä½•å˜åŒ–ã€‚

![image-20220903172258429](https://itlab1024-1256529903.cos.ap-beijing.myqcloud.com/202209031722623.png)

å¯ä»¥çœ‹åˆ°å¢åŠ äº†ä¸€ä¸ªworkerã€‚

å†æŠŠspark-standalone4åŠ å…¥è¿›æ¥

```shell
[vagrant@spark-standalone4 spark-standalone]$ sbin/start-worker.sh spark://spark-standalone1:7077 -h spark-standalone4 --webui-port 8081
starting org.apache.spark.deploy.worker.Worker, logging to /home/vagrant/spark-standalone/logs/spark-vagrant-org.apache.spark.deploy.worker.Worker-1-spark-standalone4.out
```

æŸ¥çœ‹WebUI

![image-20220903172506133](https://itlab1024-1256529903.cos.ap-beijing.myqcloud.com/202209031725226.png)

ä¹ŸæˆåŠŸåŠ å…¥äº†è¿›æ¥ã€‚

### æ­å»ºï¼ˆè„šæœ¬å¯åŠ¨ï¼‰

ä¸Šé¢çš„å¯åŠ¨æ–¹å¼æ˜¯å…ˆå¯åŠ¨masterï¼Œç„¶åä¸€ä¸ªä¸€ä¸ªworkderçš„å¯åŠ¨ï¼Œè¿™åœ¨èŠ‚ç‚¹å°‘çš„æƒ…å†µä¸‹è¿˜å¯ä»¥ï¼Œä½†æ˜¯èŠ‚ç‚¹å¤šçš„æƒ…å†µä¸‹å°±éº»çƒ¦äº†ã€‚

ä½¿ç”¨è„šæœ¬å¯åŠ¨ä¸»è¦æ˜¯æ˜¯å¦‚ä¸‹æ­¥éª¤ï¼š

A. Masteré€šè¿‡SSHæ§åˆ¶WorkerèŠ‚ç‚¹ï¼Œæ‰€ä»¥è¦ä¿è¯å„ä¸ªèŠ‚ç‚¹èƒ½å¤Ÿé€šè¿‡sshè®¿é—®ï¼Œå¹¶ä¸”æ— å¯†ç æˆ–è€…é€šè¿‡ç§é’¥è®¿é—®ã€‚

B. é…ç½®confä¸‹çš„spark-env.shï¼Œåœ¨è¿™é‡Œé…ç½®MasterèŠ‚ç‚¹çš„ä¿¡æ¯ï¼ˆåŒ…æ‹¬ä¸»æœºåã€ç«¯å£ç­‰ï¼‰ï¼Œé»˜è®¤æ²¡æœ‰è¿™ä¸ªæ–‡ä»¶ï¼Œåªéœ€è¦å°†å…¶ä¸‹çš„spark-env.sh.templateé‡å‘½åä¸ºspark-env.shå³å¯ã€‚

C. confä¸‹çš„workers.templateé‡å‘½åä¸ºworkersï¼Œåœ¨è¿™é‡Œé…ç½®æ‰€æœ‰workderçš„ä¸»æœºåã€‚

æ¥ä¸‹æ¥æ¥æ“ä½œä¸‹ã€‚

é…ç½®å…å¯†ï¼š

å››ä¸ªèŠ‚ç‚¹éœ€è¦é…ç½®å…è®¸å¯†ç ç™»å½•

```shell
sudo vi /etc/ssh/sshd_config
```

å°†é‡Œé¢çš„PasswordAuthentication noä¿®æ”¹ä¸ºPasswordAuthentication yesã€‚

![image-20220903214717899](https://itlab1024-1256529903.cos.ap-beijing.myqcloud.com/202209032147241.png)

ç„¶åé‡å¯ssdæœåŠ¡

```shell
[vagrant@spark-standalone4 ~]$ systemctl restart sshd
==== AUTHENTICATING FOR org.freedesktop.systemd1.manage-units ===
Authentication is required to manage system services or units.
Authenticating as: root
Password: 
==== AUTHENTICATION COMPLETE ===
```

åœ¨ä¸»èŠ‚ç‚¹ï¼ˆspark-standalone1ï¼‰ä½¿ç”¨å¦‚ä¸‹å‘½ä»¤ç”Ÿæˆå…¬ç§é’¥

```shell
[vagrant@spark-standalone1 ~]$ ssh-keygen -t rsa
Generating public/private rsa key pair.
Enter file in which to save the key (/home/vagrant/.ssh/id_rsa): 
Enter passphrase (empty for no passphrase): 
Enter same passphrase again: 
Your identification has been saved in /home/vagrant/.ssh/id_rsa.
Your public key has been saved in /home/vagrant/.ssh/id_rsa.pub.
The key fingerprint is:
SHA256:3xWV7mKV24HBpVNzPIwrmvFde7nZe3gEy+k/GKdUk4A vagrant@spark-standalone4
The key's randomart image is:
+---[RSA 2048]----+
|            o +=+|
|           E =o=+|
|             oB +|
|         . . oo@ |
|        S = o.*+B|
|         + o B+*+|
|          . +.*o=|
|             oo++|
|               +=|
+----[SHA256]-----+
```

ä¸€è·¯å›è½¦å³å¯

ç„¶åå°†å…¶æ‹·è´åˆ°å…¶ä»–æœåŠ¡å™¨

```
[vagrant@spark-standalone4 ~]$ ssh-copy-id vagrant@spark-standalone1
[vagrant@spark-standalone4 ~]$ ssh-copy-id vagrant@spark-standalone2
[vagrant@spark-standalone4 ~]$ ssh-copy-id vagrant@spark-standalone3
[vagrant@spark-standalone4 ~]$ ssh-copy-id vagrant@spark-standalone4
```

ä¼šè®©è¾“å…¥å¯†ç ï¼Œæˆ‘ä½¿ç”¨çš„æ˜¯vagrantå·¥å…·ï¼Œé»˜è®¤å¯†ç æ˜¯vagrantã€‚

**ç‰¹åˆ«æé†’** ï¼šå¦‚æœä½¿ç”¨çš„æ˜¯rootç”¨æˆ·ï¼Œ/etc/ssh/sshd_configéœ€è¦ä¿®æ”¹å…è®¸Rootç™»å½•ï¼Œé‡åˆ°é—®é¢˜é—®åº¦å¨˜ã€‚



é¦–å…ˆä¿®æ”¹spark-env.shï¼Œè¿™é‡Œæœ‰å¾ˆå¤šé…ç½®é¡¹ï¼Œæˆ‘å°±ç®€å•é…ç½®ä¸‹ä¸»è¦çš„ï¼Œä½¿å…¶èƒ½å¤Ÿæ­£å¸¸å¯åŠ¨é›†ç¾¤å³å¯ã€‚

```shell
[vagrant@spark-standalone1 conf]$ cat spark-env.sh 
#!/usr/bin/env bash

#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

# This file is sourced when running various Spark programs.
# Copy it as spark-env.sh and edit that to configure Spark for your site.

# Options read when launching programs locally with
# ./bin/run-example or ./bin/spark-submit
# - HADOOP_CONF_DIR, to point Spark towards Hadoop configuration files
# - SPARK_LOCAL_IP, to set the IP address Spark binds to on this node
# - SPARK_PUBLIC_DNS, to set the public dns name of the driver program

# Options read by executors and drivers running inside the cluster
# - SPARK_LOCAL_IP, to set the IP address Spark binds to on this node
# - SPARK_PUBLIC_DNS, to set the public DNS name of the driver program
# - SPARK_LOCAL_DIRS, storage directories to use on this node for shuffle and RDD data
# - MESOS_NATIVE_JAVA_LIBRARY, to point to your libmesos.so if you use Mesos

# Options read in any mode
# - SPARK_CONF_DIR, Alternate conf dir. (Default: ${SPARK_HOME}/conf)
# - SPARK_EXECUTOR_CORES, Number of cores for the executors (Default: 1).
# - SPARK_EXECUTOR_MEMORY, Memory per Executor (e.g. 1000M, 2G) (Default: 1G)
# - SPARK_DRIVER_MEMORY, Memory for Driver (e.g. 1000M, 2G) (Default: 1G)

# Options read in any cluster manager using HDFS
# - HADOOP_CONF_DIR, to point Spark towards Hadoop configuration files

# Options read in YARN client/cluster mode
# - YARN_CONF_DIR, to point Spark towards YARN configuration files when you use YARN

# Options for the daemons used in the standalone deploy mode
# - SPARK_MASTER_HOST, to bind the master to a different IP address or hostname
# - SPARK_MASTER_PORT / SPARK_MASTER_WEBUI_PORT, to use non-default ports for the master
# - SPARK_MASTER_OPTS, to set config properties only for the master (e.g. "-Dx=y")
# - SPARK_WORKER_CORES, to set the number of cores to use on this machine
# - SPARK_WORKER_MEMORY, to set how much total memory workers have to give executors (e.g. 1000m, 2g)
# - SPARK_WORKER_PORT / SPARK_WORKER_WEBUI_PORT, to use non-default ports for the worker
# - SPARK_WORKER_DIR, to set the working directory of worker processes
# - SPARK_WORKER_OPTS, to set config properties only for the worker (e.g. "-Dx=y")
# - SPARK_DAEMON_MEMORY, to allocate to the master, worker and history server themselves (default: 1g).
# - SPARK_HISTORY_OPTS, to set config properties only for the history server (e.g. "-Dx=y")
# - SPARK_SHUFFLE_OPTS, to set config properties only for the external shuffle service (e.g. "-Dx=y")
# - SPARK_DAEMON_JAVA_OPTS, to set config properties for all daemons (e.g. "-Dx=y")
# - SPARK_DAEMON_CLASSPATH, to set the classpath for all daemons
# - SPARK_PUBLIC_DNS, to set the public dns name of the master or workers

# Options for launcher
# - SPARK_LAUNCHER_OPTS, to set config properties and Java options for the launcher (e.g. "-Dx=y")

# Generic options for the daemons used in the standalone deploy mode
# - SPARK_CONF_DIR      Alternate conf dir. (Default: ${SPARK_HOME}/conf)
# - SPARK_LOG_DIR       Where log files are stored.  (Default: ${SPARK_HOME}/logs)
# - SPARK_LOG_MAX_FILES Max log files of Spark daemons can rotate to. Default is 5.
# - SPARK_PID_DIR       Where the pid file is stored. (Default: /tmp)
# - SPARK_IDENT_STRING  A string representing this instance of spark. (Default: $USER)
# - SPARK_NICENESS      The scheduling priority for daemons. (Default: 0)
# - SPARK_NO_DAEMONIZE  Run the proposed command in the foreground. It will not output a PID file.
# Options for native BLAS, like Intel MKL, OpenBLAS, and so on.
# You might get better performance to enable these options if using native BLAS (see SPARK-21305).
# - MKL_NUM_THREADS=1        Disable multi-threading of Intel MKL
# - OPENBLAS_NUM_THREADS=1   Disable multi-threading of OpenBLAS
# è¿™é‡Œæˆ‘å°±é…ç½®äº†masterçš„ä¿¡æ¯ï¼Œworkerä½¿ç”¨é»˜è®¤çš„
SPARK_MASTER_HOST=spark-standalone1
SPARK_MASTER_PORT=7077
SPARK_MASTER_WEBUI_PORT=8080
```

conf/workersæ–‡ä»¶

```shell
[vagrant@spark-standalone1 conf]$ cat workers 
#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

# A Spark Worker will be started on each of the machines listed below.
spark-standalone3
spark-standalone4
```

**ç‰¹åˆ«æ³¨æ„**ï¼Œè¿™ä¸¤ä¸ªæ–‡ä»¶è¦åŒæ­¥åˆ°æ‰€æœ‰èŠ‚ç‚¹ã€‚

**å¯åŠ¨é›†ç¾¤**ï¼Œæ‰§è¡Œsbin/start-all.sh

```shell
[vagrant@spark-standalone1 spark-standalone]$ sbin/start-all.sh 
starting org.apache.spark.deploy.master.Master, logging to /home/vagrant/spark-standalone/logs/spark-vagrant-org.apache.spark.deploy.master.Master-1-spark-standalone1.out
spark-standalone3: starting org.apache.spark.deploy.worker.Worker, logging to /home/vagrant/spark-standalone/logs/spark-vagrant-org.apache.spark.deploy.worker.Worker-1-spark-standalone3.out
spark-standalone4: starting org.apache.spark.deploy.worker.Worker, logging to /home/vagrant/spark-standalone/logs/spark-vagrant-org.apache.spark.deploy.worker.Worker-1-spark-standalone4.out
```

ä»æ—¥å¿—ä¸Šçœ‹ä¸€ä¸ªä¸»ï¼ˆspark-standalone1ï¼‰,ä¸¤ä¸ªä»ï¼ˆspark-standalone3å’Œspark-standalone4ï¼‰éƒ½å·²ç»å¯åŠ¨ï¼ŒæŸ¥çœ‹ä¸‹UIçœ‹çœ‹æ˜¯å¦å¯åŠ¨æˆåŠŸã€‚

![image-20220903220549261](https://itlab1024-1256529903.cos.ap-beijing.myqcloud.com/202209032205543.png)

æ²¡æœ‰é—®é¢˜ï¼

### æäº¤åº”ç”¨

ä½¿ç”¨spark-submitè„šæœ¬æäº¤åº”ç”¨

```shell
[vagrant@spark-standalone1 spark-standalone]$ bin/spark-submit --class org.apache.spark.examples.SparkPi --master spark://spark-standalone1:7077 ./examples/jars/spark-examples_2.12-3.3.0.jar 10
22/09/03 14:09:47 INFO SparkContext: Running Spark version 3.3.0
22/09/03 14:09:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
22/09/03 14:09:47 INFO ResourceUtils: ==============================================================
22/09/03 14:09:47 INFO ResourceUtils: No custom resources configured for spark.driver.
22/09/03 14:09:47 INFO ResourceUtils: ==============================================================
22/09/03 14:09:47 INFO SparkContext: Submitted application: Spark Pi
22/09/03 14:09:47 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
22/09/03 14:09:48 INFO ResourceProfile: Limiting resource is cpu
-----------çœç•¥ä¸€éƒ¨åˆ†æ—¥å¿—-------------
```

å†æ¬¡æŸ¥çœ‹WebUI

![image-20220903221049928](https://itlab1024-1256529903.cos.ap-beijing.myqcloud.com/202209032210162.png)

Goodï¼ï¼ï¼

### å†å²æœåŠ¡

Sparkæœ‰WebUIç›‘æ§ï¼Œä½†æ˜¯ä¸€æ—¦é‡å¯æœåŠ¡å™¨ï¼Œå†å²å°±ä¼šä¸¢å¤±ï¼ŒSparkæä¾›å†å²æœåŠ¡ï¼Œéœ€è¦å°†æ•°æ®ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶æˆ–è€…HDFSï¼ˆHadoop åˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿï¼‰ä¸­ã€‚

HDFSæ­å»ºè¿™é‡Œæˆ‘å°±ä¸æ¼”ç¤ºäº†ï¼Œè‡ªè¡ŒæŸ¥çœ‹[Hadoopå®˜ç½‘]( https://hadoop.apache.org/)æ–‡æ¡£æ­å»ºã€‚

é…ç½®conf/spark-defaults.confæ–‡ä»¶

```text
[vagrant@spark-standalone1 conf]$ cat spark-defaults.conf
#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

# Default system properties included when running spark-submit.
# This is useful for setting default environmental settings.

# Example:
spark.master                     spark://spark-standalone1:7077
spark.eventLog.enabled           true
spark.eventLog.dir               hdfs://spark-standalone1:9000/spark-events
# spark.serializer                 org.apache.spark.serializer.KryoSerializer
# spark.driver.memory              5g
# spark.executor.extraJavaOptions  -XX:+PrintGCDetails -Dkey=value -Dnumbers="one two three
```

ä¸»è¦ä¿®æ”¹äº†ä¸€ä¸‹ä¸‰å¤„

```text
spark.master                     spark://spark-standalone1:7077
spark.eventLog.enabled           true
spark.eventLog.dir               hdfs://spark-standalone1:9000/spark-events
```

ç„¶åä¿®æ”¹conf/spark-env.sh

```
[vagrant@spark-standalone1 conf]$ cat spark-env.sh 
#!/usr/bin/env bash

#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

# This file is sourced when running various Spark programs.
# Copy it as spark-env.sh and edit that to configure Spark for your site.

# Options read when launching programs locally with
# ./bin/run-example or ./bin/spark-submit
# - HADOOP_CONF_DIR, to point Spark towards Hadoop configuration files
# - SPARK_LOCAL_IP, to set the IP address Spark binds to on this node
# - SPARK_PUBLIC_DNS, to set the public dns name of the driver program

# Options read by executors and drivers running inside the cluster
# - SPARK_LOCAL_IP, to set the IP address Spark binds to on this node
# - SPARK_PUBLIC_DNS, to set the public DNS name of the driver program
# - SPARK_LOCAL_DIRS, storage directories to use on this node for shuffle and RDD data
# - MESOS_NATIVE_JAVA_LIBRARY, to point to your libmesos.so if you use Mesos

# Options read in any mode
# - SPARK_CONF_DIR, Alternate conf dir. (Default: ${SPARK_HOME}/conf)
# - SPARK_EXECUTOR_CORES, Number of cores for the executors (Default: 1).
# - SPARK_EXECUTOR_MEMORY, Memory per Executor (e.g. 1000M, 2G) (Default: 1G)
# - SPARK_DRIVER_MEMORY, Memory for Driver (e.g. 1000M, 2G) (Default: 1G)

# Options read in any cluster manager using HDFS
# - HADOOP_CONF_DIR, to point Spark towards Hadoop configuration files

# Options read in YARN client/cluster mode
# - YARN_CONF_DIR, to point Spark towards YARN configuration files when you use YARN

# Options for the daemons used in the standalone deploy mode
# - SPARK_MASTER_HOST, to bind the master to a different IP address or hostname
# - SPARK_MASTER_PORT / SPARK_MASTER_WEBUI_PORT, to use non-default ports for the master
# - SPARK_MASTER_OPTS, to set config properties only for the master (e.g. "-Dx=y")
# - SPARK_WORKER_CORES, to set the number of cores to use on this machine
# - SPARK_WORKER_MEMORY, to set how much total memory workers have to give executors (e.g. 1000m, 2g)
# - SPARK_WORKER_PORT / SPARK_WORKER_WEBUI_PORT, to use non-default ports for the worker
# - SPARK_WORKER_DIR, to set the working directory of worker processes
# - SPARK_WORKER_OPTS, to set config properties only for the worker (e.g. "-Dx=y")
# - SPARK_DAEMON_MEMORY, to allocate to the master, worker and history server themselves (default: 1g).
# - SPARK_HISTORY_OPTS, to set config properties only for the history server (e.g. "-Dx=y")
# - SPARK_SHUFFLE_OPTS, to set config properties only for the external shuffle service (e.g. "-Dx=y")
# - SPARK_DAEMON_JAVA_OPTS, to set config properties for all daemons (e.g. "-Dx=y")
# - SPARK_DAEMON_CLASSPATH, to set the classpath for all daemons
# - SPARK_PUBLIC_DNS, to set the public dns name of the master or workers

# Options for launcher
# - SPARK_LAUNCHER_OPTS, to set config properties and Java options for the launcher (e.g. "-Dx=y")

# Generic options for the daemons used in the standalone deploy mode
# - SPARK_CONF_DIR      Alternate conf dir. (Default: ${SPARK_HOME}/conf)
# - SPARK_LOG_DIR       Where log files are stored.  (Default: ${SPARK_HOME}/logs)
# - SPARK_LOG_MAX_FILES Max log files of Spark daemons can rotate to. Default is 5.
# - SPARK_PID_DIR       Where the pid file is stored. (Default: /tmp)
# - SPARK_IDENT_STRING  A string representing this instance of spark. (Default: $USER)
# - SPARK_NICENESS      The scheduling priority for daemons. (Default: 0)
# - SPARK_NO_DAEMONIZE  Run the proposed command in the foreground. It will not output a PID file.
# Options for native BLAS, like Intel MKL, OpenBLAS, and so on.
# You might get better performance to enable these options if using native BLAS (see SPARK-21305).
# - MKL_NUM_THREADS=1        Disable multi-threading of Intel MKL
# - OPENBLAS_NUM_THREADS=1   Disable multi-threading of OpenBLAS
SPARK_MASTER_HOST=spark-standalone1
SPARK_MASTER_PORT=7077
SPARK_MASTER_WEBUI_PORT=8080
SPARK_HISTORY_OPTS="-Dspark.history.retainedApplications=3 
-Dspark.history.fs.logDirectory=hdfs://spark-standalone1:9000/spark-events"
```

å¢åŠ äº†å¦‚ä¸‹é…ç½®ï¼ˆå†å²æœåŠ¡ï¼‰

```text
SPARK_HISTORY_OPTS="-Dspark.history.retainedApplications=3
-Dspark.history.fs.logDirectory=hdfs://spark-standalone1:9000/spark-events"
```

å†å²æœåŠ¡é»˜è®¤è¯·æ±‚åœ°å€æ˜¯ï¼šhttp://spark-standalone1:18080

![image-20220905103140306](https://itlab1024-1256529903.cos.ap-beijing.myqcloud.com/202209051031707.png)

æ­¤æ—¶å†å²æœåŠ¡é‡Œæ²¡æœ‰è®°å½•ï¼Œæˆ‘æ¥æäº¤ä¸€ä¸ªåº”ç”¨

```shell
[vagrant@spark-standalone1 spark-standalone]$ bin/spark-submit --class org.apache.spark.examples.SparkPi --master spark://spark-standalone1:7077 ./examples/jars/spark-examples_2.12-3.3.0.jar 10
```

é‡æ–°æŸ¥çœ‹å†å²æœåŠ¡

![image-20220905105303338](https://itlab1024-1256529903.cos.ap-beijing.myqcloud.com/202209051053652.png)

å¯ä»¥æŸ¥çœ‹åˆ°åˆšåˆšæäº¤çš„sparkè®°å½•ã€‚

æŸ¥çœ‹ä¸‹Hadoop hdfsçš„é¡µé¢

![image-20220905105613132](https://itlab1024-1256529903.cos.ap-beijing.myqcloud.com/202209051056262.png)

### é«˜å¯ç”¨

é«˜å¯ç”¨æ¨¡å¼ä¹‹åå•ç‹¬å‡ºä¸€ç¯‡æ–‡ç« ã€‚

## YARNæ¨¡å¼

ä¹‹åå­¦ä¹ å†æ›´æ–°ä¸€ä¸ªå•ç‹¬ç« èŠ‚

## Kubernetesæ¨¡å¼

ä¹‹åå­¦ä¹ å†æ›´æ–°ä¸€ä¸ªå•ç‹¬ç« èŠ‚ã€‚



# Sparkè¿è¡Œæ¶æ„

æœ¬ç« ä»‹ç»ä¸‹Sparkçš„ç»„ä»¶ä»¥åŠè¿è¡Œæ–¹å¼ç­‰ä¿¡æ¯ï¼Œè¯¥ç¯‡ç‰¹åˆ«é‡è¦ï¼Œæ— è®ºå­¦ä¹ ä»€ä¹ˆæŠ€æœ¯ï¼Œäº†è§£å…¶åº•å±‚åŸç†éƒ½æ˜¯éå¸¸é‡è¦çš„ã€‚

![Spark é›†ç¾¤ç»„ä»¶](https://itlab1024-1256529903.cos.ap-beijing.myqcloud.com/202209041819018.png)

ä¸Šå›¾å°±æ˜¯Sparkè¿è¡Œæ—¶çš„åŸºæœ¬ç»“æ„ã€‚

ä¸Šé¢å›¾ä¸­éƒ½ä»£è¡¨ä»€ä¹ˆæ„æ€ï¼Ÿæˆ‘å…ˆæ”¾ä¸€å¼ å®˜ç½‘çš„é›†ç¾¤è¯æ±‡è¡¨

| æœ¯è¯­            | å«ä¹‰                                                         |
| :-------------- | :----------------------------------------------------------- |
| Application     | åŸºäº Spark æ„å»ºçš„ç”¨æˆ·ç¨‹åºã€‚åŒ…æ‹¬é›†ç¾¤ä¸Šçš„Driverå’ŒExecutorsã€‚   |
| Application jar | åŒ…å«ç”¨æˆ·ç¨‹åºçš„JaråŒ…ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œç”¨æˆ·ä¼šå¸Œæœ›åˆ›å»ºä¸€ä¸ªè¶…çº§Jarï¼Œå…¶ä¸­åŒ…å«åº”ç”¨ç¨‹åºå’Œä¾èµ–é¡¹ï¼Œç”¨æˆ·çš„ç¨‹åºä¸åº”è¯¥åŒ…å«Hadoopæˆ–è€…Sparkçš„ä¾èµ–ï¼Œè¿™äº›Jaråº”è¯¥åœ¨Sparkè¿è¡Œæ—¶å†…ï¼Œæ”¾åˆ°Sparkçš„å®‰è£…ç›®å½•ä¸‹çš„jarsæ–‡ä»¶å¤¹ä¸‹ï¼Œé»˜è®¤å°±æœ‰å¾ˆå¤šJarã€‚ |
| Driver program  | è¿è¡Œåº”ç”¨ç¨‹åºçš„ main() å‡½æ•°å¹¶åˆ›å»º SparkContext çš„è¿›ç¨‹         |
| Cluster manager | ç”¨äºè·å–é›†ç¾¤ä¸Šèµ„æºçš„å¤–éƒ¨æœåŠ¡ï¼ˆä¾‹å¦‚standalone managerã€Mesosã€YARNã€Kubernetesï¼‰ã€‚ |
| Deploy mode     | åŒºåˆ†Driverè¿è¡Œåœ¨å“ªé‡Œçš„æ ‡å¿—ï¼Œå¦‚æœæ¨¡å¼æ˜¯â€œclusterâ€ï¼Œæ¡†æ¶åœ¨é›†ç¾¤å†…éƒ¨å¯åŠ¨Driverï¼Œå¦‚æœæ˜¯â€œclientâ€æäº¤è€…åœ¨é›†ç¾¤å¤–éƒ¨å¯åŠ¨Driverã€‚ |
| Worker node     | å¯ä»¥åœ¨é›†ç¾¤ä¸­è¿è¡Œåº”ç”¨ç¨‹åºä»£ç çš„ä»»ä½•èŠ‚ç‚¹ï¼ŒExecutorå°±åœ¨å·¥ä½œèŠ‚ç‚¹ä¸Šã€‚ |
| Executor        | ä¸ºå·¥ä½œèŠ‚ç‚¹ä¸Šçš„Applicationå¯åŠ¨çš„è¿›ç¨‹ï¼Œå®ƒè¿è¡ŒTaskå¹¶å°†æ•°æ®ä¿å­˜åœ¨å†…å­˜æˆ–ç£ç›˜å­˜å‚¨ä¸­ã€‚æ¯ä¸ªApplicationéƒ½æœ‰è‡ªå·±çš„Executorsã€‚ |
| Task            | å‘é€ä¸ªExecutorçš„å·¥ä½œå•å…ƒ                                     |
| Job             | ç”±å¤šä¸ªTaskç»„æˆçš„è¿‡ä¸ªå¹¶è¡Œè®¡ç®—ï¼Œè¿™äº›Taskå“åº”Sparkçš„Actionï¼ˆä¸€ç§ç®—å­ï¼ŒSparkä¸­æœ‰ä¸¤ç§ç®—å­ï¼Œå¦ä¸€ç§æ˜¯Transformï¼‰ï¼ŒDriverçš„æ—¥å¿—èƒ½çœ‹åˆ°Jobçš„ç›¸å…³æ—¥å¿—ä¿¡æ¯ |
| Stage           | æ¯ä¸€ä¸ªJobéƒ½ä¼šè¢«åˆ†å‰²ä¸ºæ›´å°çš„Taské›†åˆï¼ŒEach job gets divided into smaller sets of tasks called *stages* that depend on each other (similar to the map and reduce stages in MapReduce); you'll see this term used in the driver's logs. |



## æ ¸å¿ƒç»„ä»¶

Sparkæœ‰ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼ŒDriverå’ŒExecutor

### Driver

Driverç”¨äºæ‰§è¡Œåº”ç”¨ç¨‹åºçš„mainæ–¹æ³•ï¼Œä»–åœ¨ä½œä¸šæ‰§è¡Œçš„æ—¶å€™ä¸»è¦è´Ÿè´£å¦‚ä¸‹å·¥ä½œï¼š

* å°†ç”¨æˆ·ç¨‹åºè½¬åŒ–ä¸ºJob

* åœ¨Executorä¹‹é—´è°ƒåº¦ä»»åŠ¡

* è·Ÿè¸ªExecutorçš„æ‰§è¡Œæƒ…å†µ

* é€šè¿‡WebUIæŸ¥è¯¢è¿è¡Œæƒ…å†µ

Driveré»˜è®¤è¿è¡Œåœ¨æäº¤ä»»åŠ¡çš„æœºå™¨ä¸Šï¼ˆå› ä¸ºæäº¤ä»»åŠ¡é»˜è®¤æ–¹å¼ï¼ˆdeploy-modeï¼‰ä½¿ç”¨çš„æ˜¯å®¢æˆ·ç«¯æ¨¡å¼(client)ï¼‰ï¼Œå¦‚æœæ˜¯é›†ç¾¤æ¨¡å¼ï¼ˆclusterï¼‰ï¼Œåˆ™é›†ç¾¤ç®¡ç†å™¨ä¼šéšæœºé€‰æ‹©ä¸€ä¸ªworkerå¯åŠ¨Driverï¼Œä¹‹å‰æˆ‘æäº¤åº”ç”¨çš„æ–¹å¼å› ä¸ºæ²¡æœ‰æŒ‡å®šdeploy-modeå‚æ•°ï¼Œæ‰€ä»¥ä½¿ç”¨é»˜è®¤clientæ¨¡å¼ã€‚

### Executor

Executorä¹Ÿæ˜¯ä¸€ä¸ªè¿›ç¨‹ï¼Œä»–è¿è¡Œåœ¨WorkerèŠ‚ç‚¹ï¼Œè´Ÿè´£æ‰§è¡ŒSparkçš„ä»»åŠ¡ï¼Œå°†ç»“æœè¿”å›ç»™Driverï¼ŒåŒæ—¶ä»–ä¹Ÿæä¾›ä¸ºéœ€è¦ç¼“å­˜çš„RDDæä¾›å†…å­˜å­˜å‚¨ã€‚



## æäº¤ä»»åŠ¡æµç¨‹

ä»»åŠ¡æäº¤ä½¿ç”¨Sparkç›®å½•ä¸‹çš„bin/spark-submitæ‰§è¡Œï¼Œä»–æœ‰å¾ˆå¤šå‚æ•°å¯ä»¥æ‰§è¡Œï¼Œå¦‚ä¸‹è¡¨

| å‚æ•°                     | è§£é‡Š                                                         | å¯é€‰å€¼ä¸¾ä¾‹                                                |
| ------------------------ | ------------------------------------------------------------ | --------------------------------------------------------- |
| --class                  | Sparkç¨‹åºä¸­åŒ…å«ä¸»å‡½æ•°çš„ç±»å®Œå…¨å                              | --class org.apache.spark.examples.SparkPi                 |
| --master                 | Sparkç¨‹åºè¿è¡Œçš„æ¨¡å¼                                          | æœ¬åœ°æ¨¡å¼ï¼šlocal[*]ã€spark://spark-standalone1:7077 ã€Yarn |
| --deploy-mode            | æäº¤åº”ç”¨æ¨¡å¼ï¼Œclientå’Œclusterï¼Œé»˜è®¤æ˜¯clientï¼Œclientæ¨¡å¼ä¸»è¦ç”¨äºå¼€å‘å’Œæµ‹è¯•ï¼Œç”Ÿäº§ç¯å¢ƒå¿…é¡»ä½¿ç”¨cluster |                                                           |
| --executor-memory 1G     | æŒ‡å®šæ¯ä¸ªexecutorå¯ç”¨å†…å­˜ä¸º1G                                 | ç¬¦åˆé›†ç¾¤å†…å­˜é…ç½®å³å¯ï¼Œå…·ä½“æƒ…å†µå…·ä½“åˆ†æã€‚                  |
| --total-executor-cores 2 | æŒ‡å®šæ‰€æœ‰executorä½¿ç”¨çš„cpuæ ¸æ•°ä¸º2ä¸ª                           |                                                           |
| application-jar          | æ‰“åŒ…å¥½çš„åº”ç”¨jarï¼ŒåŒ…å«ä¾èµ–ã€‚è¿™ä¸ªURLåœ¨é›†ç¾¤ä¸­å…¨å±€å¯è§ã€‚ æ¯”å¦‚hdfs:// å…±äº«å­˜å‚¨ç³»ç»Ÿï¼Œå¦‚æœæ˜¯file:// pathï¼Œé‚£ä¹ˆæ‰€æœ‰çš„èŠ‚ç‚¹çš„pathéƒ½åŒ…å«åŒæ ·çš„jar |                                                           |
| application-arguments    | ä¼ ç»™main()æ–¹æ³•çš„å‚æ•°                                         |                                                           |

æ¥ä¸‹æ¥æˆ‘ä¸»è¦ä»‹ç»ä¸‹å®¢æˆ·ç«¯æ¨¡å¼å’Œé›†ç¾¤æ¨¡å¼ä¸‹ä»»åŠ¡æäº¤çš„æµç¨‹ã€‚

### å®¢æˆ·ç«¯æ¨¡å¼

è¿™æ˜¯é»˜è®¤çš„æ¨¡å¼ï¼Œå€Ÿç”¨ä¸€å¼ ç½‘ç»œå›¾æ¥è¯´æ˜æäº¤æµç¨‹ã€‚

![img](https://itlab1024-1256529903.cos.ap-beijing.myqcloud.com/202209081603802.png)

ç”¨æˆ·åœ¨æœ¬åœ°æœºå™¨ä¸Šæ‰§è¡Œbin/submitè„šæœ¬åï¼Œä¼šåœ¨æœ¬æœºä¸Šå¯åŠ¨ä¸€ä¸ªJVMè¿›ç¨‹ï¼Œå°±æ˜¯Driverï¼ŒDriverè§£æï¼ˆè½¬åŒ–ä¸ºJobç­‰ï¼‰åº”ç”¨åå°†å…¶æ³¨å†Œåˆ°masterï¼ŒMasteræ ¹æ®èµ„æºçš„éœ€æ±‚è·å–workerèµ„æºï¼ˆå¯åŠ¨Executorè¿›ç¨‹ï¼‰ï¼Œç„¶åExecutorä¼šåå‘æ³¨å†Œç»™Driverï¼Œä¹‹åDriverä¼šå°†Taskåˆ†é…ç»™å…·ä½“çš„Executoræ‰§è¡Œï¼Œæ‰§è¡Œå®Œæ¯•ååExecutorä¼šå°†ç»“æœåé¦ˆç»™Driverã€‚

### é›†ç¾¤æ¨¡å¼

![img](https://itlab1024-1256529903.cos.ap-beijing.myqcloud.com/202209081600166.jpeg)

é›†ç¾¤æ¨¡å¼æäº¤æµç¨‹è·Ÿå®¢æˆ·ç«¯æ¨¡å¼æ˜¯ç±»ä¼¼çš„ï¼Œä¸åŒçš„æ˜¯Driverçš„æ‰§è¡Œåœ°ç‚¹ï¼Œå®¢æˆ·ç«¯æ˜¯åœ¨æäº¤åº”ç”¨çš„é‚£ä¸ªæœºå™¨ä¸Šå¯åŠ¨Driverï¼Œé›†ç¾¤æ¨¡å¼ä¸‹ï¼Œæ˜¯Masteréšæœºæ‰¾ä¸€ä¸ªWorkerè¿è¡ŒDriverã€‚å…¶ä»–æ²¡æœ‰ä»€ä¹ˆåŒºåˆ«ã€‚



è¿™æ ·åšçš„ç›®çš„æ— éå°±æ˜¯åœ¨æäº¤ä»»åŠ¡å¤šçš„æ—¶å€™ï¼Œé€šè¿‡å¤šworkerçš„ç‰¹ç‚¹å°†å‹åŠ›å‡å°ã€‚è¯•æƒ³å®¢æˆ·ç«¯æ¨¡å¼ä¸‹ï¼Œå¦‚æœä»»åŠ¡è¿‡å¤šï¼Œå°±ä¼šå¯åŠ¨å¾ˆå¤šè¿›ç¨‹ï¼Œè¿™æ— ç–‘ä¼šå¢åŠ è®¡ç®—æœºçš„è´Ÿæ‹…ã€‚



# æ ¸å¿ƒç¼–ç¨‹

RDDï¼ˆResilient Distributed Datasetsï¼‰ï¼šå¼¹æ€§åˆ†å¸ƒå¼æ•°æ®é›†ï¼Œä»–æ°¸è¿œæ˜¯ä¸€ä¸ªé›†åˆï¼Œä»–æ˜¯Sparkçš„æ ¸å¿ƒéƒ¨åˆ†ï¼ŒSpark-SQLç­‰ä¸Šå±‚æ¶æ„éƒ½æ˜¯åŸºäºRDDæ¥å®ç°çš„ã€‚

RDDæ•°æ®æ˜¯åˆ†å¸ƒå¼å­˜å‚¨çš„ï¼Œä¹Ÿå°±æ˜¯æŒ‰ç…§ä¸åŒçš„åˆ†åŒºå­˜å‚¨ã€‚å¯ä»¥æ ¹æ®å†…ç½®çš„æ–¹æ³•è‡ªç”±æ‰©å±•åˆ†åŒºæˆ–è€…ç¼©å°åˆ†åŒºï¼ˆçœ‹å®é™…ä¸šåŠ¡æƒ…å†µï¼‰ã€‚

## åˆè¯†ç®—å­

RDDæœ‰å¾ˆå¤šæ–¹æ³•ï¼Œè¿™äº›æ–¹æ³•å«åšç®—å­ï¼Œç®—å­ä¸»è¦åˆ†ä¸ºä¸¤ç§ï¼Œä¸€ç§æ˜¯è½¬æ¢ï¼ˆTransformationsï¼‰ï¼Œä¸€ç§æ˜¯åŠ¨ä½œ(Action)ã€‚

è½¬æ¢ç®—å­ä¸»è¦ç”¨äºå®šä¹‰RDDå¤„ç†æµç¨‹ï¼Œæ¯”å¦‚mapï¼ŒflatMapç­‰ç­‰ã€‚

åŠ¨ä½œç®—å­ç”¨äºè§¦å‘æ‰§è¡Œï¼Œå› ä¸ºSparkä¸­çš„ä»»åŠ¡æ‰§è¡Œæ˜¯æƒ°æ€§çš„ï¼Œåªæœ‰è§¦å‘åŠ¨ä½œç®—å­çš„æ—¶å€™æ‰ä¼šçœŸæ­£çš„è®¡ç®—ï¼Œæ¯”å¦‚

reduceByKeyç­‰ç­‰ã€‚

## DAGï¼ˆDirected Acyclic Graphï¼‰æœ‰å‘æ— ç¯å›¾

é¡¾åæ€ä¹‰å°±æ˜¯ä¸€ä¸ªæœ‰æ–¹å‘çš„ä½†æ˜¯ä¸èƒ½å½¢æˆé—­ç¯çš„å›¾ï¼Œè¿™å°±æ˜¯è¯´RDDæ‰€æœ‰çš„ç®—å­éƒ½éµå¾ªè¿™æ ·çš„è§„èŒƒã€‚

æˆ‘æ‰“ç®—ä½¿ç”¨ä¹‹å‰çš„wordcountä¾‹å­æ¥å…·ä½“è®²è§£ä¸‹DAG

ä¸ºäº†æ–¹ä¾¿ï¼Œæˆ‘å…ˆè¯•ç”¨sparkæ‰§è¡ŒwordCountä»£ç ã€‚

```shell
itlab@itlab1024com ~/dev-tools/spark-3.3.0-bin-hadoop3$ bin/spark-shell        
22/09/08 16:32:08 WARN Utils: Your hostname, itlab1024com.local resolves to a loopback address: 127.0.0.1; using 10.112.82.59 instead (on interface en0)
22/09/08 16:32:08 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
22/09/08 16:32:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Spark context Web UI available at http://10.112.82.59:4040
Spark context available as 'sc' (master = local[*], app id = local-1662625937869).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.3.0
      /_/
         
Using Scala version 2.12.15 (Java HotSpot(TM) 64-Bit Server VM, Java 17.0.3.1)
Type in expressions to have them evaluated.
Type :help for more information.

scala>     //1. å°†æ–‡ä»¶ä¸­çš„æ•°æ®è¯»å…¥åˆ°å†…å­˜ï¼Œç»“æœæ˜¯ä¸€è¡Œä¸€è¡Œçš„ã€‚

scala>     val rdd = sc.textFile("files/wordCount.txt")
rdd: org.apache.spark.rdd.RDD[String] = files/wordCount.txt MapPartitionsRDD[1] at textFile at <console>:23

scala>     //2. å°†æ¯è¡Œé€šè¿‡ç©ºæ ¼åˆ‡åˆ†

scala>     val flatRDD = rdd.flatMap(_.split(" "))
flatRDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[2] at flatMap at <console>:23

scala>     //3. è½¬åŒ–ä¸ºå…ƒç»„ï¼Œæ¯”å¦‚(K,V)ï¼ŒKä»£è¡¨å•è¯ï¼ŒVä»£è¡¨å•è¯çš„æ•°é‡ï¼ˆå†™æ­»1ï¼‰

scala>     val tupleRDD = flatRDD.map((_, 1))
tupleRDD: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[3] at map at <console>:23

scala>     //4. ç„¶åé€šè¿‡Kèšåˆå°†æ‰€æœ‰VåŠ èµ·

scala>     val result = tupleRDD.reduceByKey(_ + _)
result: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[4] at reduceByKey at <console>:23

scala>     // æ‰“å°

scala>     result.collect().foreach(println)
(scala,1)
(learning,4)
(am,4)
(java,1)
(go,1)
(spark,1)
(I,4)
```

ç„¶åæ‰“å¼€WebUIã€‚

![image-20220908163421293](https://itlab1024-1256529903.cos.ap-beijing.myqcloud.com/202209081634639.png)

è¿›å…¥åå†ç‚¹å‡»

![image-20220908163509088](https://itlab1024-1256529903.cos.ap-beijing.myqcloud.com/202209081635258.png)

è¿™ä¸ªå›¾å°±æ˜¯DAGã€‚

çœ‹åˆ°è¿™å°±ä¼šæœ‰å¾ˆå¤šç–‘é—®ï¼Œå›¾1ä¸­çš„jobæ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿæœ‰job0ï¼Œä¼šæœ‰job1å—ï¼Ÿå›¾2ä¸ºä»€ä¹ˆæ˜¯è¿™æ ·çš„ï¼Ÿstageæ˜¯ä»€ä¹ˆï¼Ÿå¦‚ä½•åˆ’åˆ†çš„ï¼ŸTaskæ˜¯å¦‚ä½•åˆ’åˆ†çš„ã€‚

Sparkå¯¹åº”ç”¨è¿›è¡Œå¤„ç†ï¼Œå¯ä»¥åˆ†è§£ä¸ºå¤šä¸ªJobï¼Œä¾æ®ä¸»è¦æ˜¯æ ¹æ®Actionç®—å­ï¼Œé‡åˆ°Actionç®—å­å°±ä¼šæ‹†è§£ä¸ºjobï¼Œè€Œæ¯ä¸€ä¸ªJobä¸­å¦‚æœé‡åˆ°Shuffleç®—å­ï¼ˆæ´—ç‰Œç®—å­ï¼Œæ•°æ®ä¼šé‡æ–°åˆ†åŒºï¼‰ï¼Œå°±ä¼šæ‹†è§£ä¸ºstageï¼Œsparkä¼šå°†æŸä¸ªæˆ–è€…æŸäº›ç®—å­æ”¾åˆ°ä¸€èµ·ç»„è£…ä¸ºä¸€ä¸ªTaskï¼ˆSparkè‡ªèº«æœ‰ä¼˜åŒ–ï¼Œä¼šå°†æŸäº›ç®—å­æ”¾åˆ°ä¸€èµ·ï¼Œæ‹†åˆ†è§„åˆ™æˆ‘æš‚æ—¶ä¸æ¸…æ¥šï¼‰ï¼Œæœ€ç»ˆçš„taskä¼šå‘é€åˆ°Executeræ‰§è¡Œï¼ŒTaskä¹Ÿæ˜¯Sparkçš„æœ€å°æ‰§è¡Œå•å…ƒã€‚

## åˆ›å»ºRDD

RDDçš„åˆ›å»ºæ–¹å¼æœ‰å¾ˆå¤š

* é€šè¿‡é›†åˆåˆ›å»º
* é€šè¿‡å¤–éƒ¨æ•°æ®æºåˆ›å»º

### é€šè¿‡é›†åˆåˆ›å»º

Sparkæ”¯æŒåœ¨å†…å­˜ä¸­é€šè¿‡é›†åˆåˆ›å»ºRDDå¯ä»¥é€šè¿‡parallelizeå’ŒmakeRDDæ–¹æ³•ï¼Œè¿™ä¸¤ä¸ªæ–¹æ³•æ˜¯å®Œå…¨ä¸€æ ·çš„ï¼Œå› ä¸ºmakeRDDåº•å±‚è°ƒç”¨çš„å°±æ˜¯parallelizeã€‚æ¨èä½¿ç”¨makeRDDï¼Œè¿™ä¸ªåå­—å®¹æ˜“ç†è§£ã€‚

```scala
package com.itlab1024.spark.core

import org.apache.logging.slf4j.Log4jLoggerFactory
import org.apache.spark.{SparkConf, SparkContext}

/**
 * åˆ›å»ºRDD
 *
 * @author itlab1024
 */
object RDDCreate01 {
  def main(args: Array[String]): Unit = {
    // å®šä¹‰é…ç½®ï¼Œé€šè¿‡é…ç½®å»ºç«‹è¿æ¥
    val conf = new SparkConf().setAppName("åº”ç”¨").setMaster("local[*]")
    val sc = new SparkContext(conf)

    // å‡†å¤‡å†…å­˜Seqæ•°æ®
    val list = List(1, 2, 3, 4)
    // é€šè¿‡parallelizeæ–¹æ³•
    val intRDD = sc.parallelize(list)
    intRDD.foreach(println)
    // é€šè¿‡makeRDDæ–¹æ³•
    val intRDD2 = sc.makeRDD(list)
    intRDD2.foreach(println)

    // å…³é—­è¿æ¥
    sc.stop()
  }
}
```

è¿™ä¸¤ä¸ªæ–¹æ³•é™¤äº†ç¬¬ä¸€ä¸ªSeqç±»å‹çš„å‚æ•°å¤–ï¼Œè¿˜æœ‰ç¬¬äºŒä¸ªå‚æ•°numSlicesï¼Œå¹¶è¡Œåº¦çš„æ¦‚å¿µï¼Œæ„æ€æ˜¯è®²é›†åˆåˆ†é…åˆ°å‡ ä¸ªåˆ†åŒºï¼Œæ¥æµ‹è¯•ä¸‹ã€‚

æµ‹è¯•åˆ†åŒºéœ€è¦ä½¿ç”¨ä¸€ä¸ªè¾“å‡ºæ–¹æ³•ï¼Œå°†å…¶ä¿å­˜åˆ°åˆ†åŒºæ–‡ä»¶ä¸­ï¼Œæ›´èƒ½ç›´è§‚çš„å±•ç¤ºã€‚

```scala
//ä½¿ç”¨saveAsTextFileå°†æ•°æ®ä»¥æ–‡ä»¶çš„å½¢å¼ä¿å­˜åˆ°ä¸åŒåˆ†åŒºï¼Œæ”¾åˆ°é¡¹ç›®ä¸‹çš„partitionsæ–‡ä»¶å¤¹ä¸‹
val intRDD3 = sc.makeRDD(list, 2)
intRDD3.saveAsTextFile("partitions")
```

æŸ¥çœ‹partitionsæ–‡ä»¶å¤¹ï¼Œå¯ä»¥çœ‹åˆ°ä¸¤ä¸ªåˆ†åŒºæ–‡ä»¶ï¼Œå¹¶ä¸”æ•°æ®å·²ç»æ”¾å…¥æ–‡ä»¶ä¸­ã€‚

![image-20220908201132328](https://itlab1024-1256529903.cos.ap-beijing.myqcloud.com/202209082011590.png)

ä¸¤ä¸ªåˆ†åŒºï¼Œ[1 2]è¢«æ”¾åˆ°äº†ç¬¬ä¸€ä¸ªåˆ†åŒºï¼Œ[3 4]è¢«æ”¾åˆ°äº†ç¬¬äºŒä¸ªåˆ†åŒºã€‚

### é€šè¿‡å¤–éƒ¨æ•°æ®æºåˆ›å»ºRDD

Sparkæ”¯æŒæœ¬åœ°æ–‡ä»¶ç³»ç»Ÿã€Hadoopæ”¯æŒçš„æ•°æ®é›†ï¼ˆæ¯”å¦‚HDFSã€Hbaseç­‰ï¼‰æ¥åˆ›å»ºRDDï¼Œè¿™é‡Œæœ‰åŸºç¡€ç‰¹åˆ«éœ€è¦æ³¨æ„çš„åœ°æ–¹

* ä½¿ç”¨æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿè·¯å¾„ï¼Œå¿…é¡»èƒ½å¤Ÿåœ¨æ¯ä¸€ä¸ªworkerä¸Šéƒ½èƒ½è®¿é—®åˆ°æ–‡ä»¶ï¼Œå¦åˆ™å°±æ‰¾ä¸åˆ°æ–‡ä»¶æ— æ³•å®Œæˆä»»åŠ¡
* æ”¯æŒç›®å½•ï¼Œå‹ç¼©æ–‡ä»¶å’Œé€šé…ç¬¦æ–¹å¼ï¼Œæ¯”å¦‚ textFile("/my/directory")ï¼Œ textFile("/my/directory/*.txt")å’ŒtextFile("/my/directory/*.gz")ç­‰ï¼Œå¤šä¸ªæ–‡ä»¶æ—¶åˆ†åŒºé¡ºåºæ— æ³•ä¿è¯ï¼Œå–å†³äºæ–‡ä»¶ç³»ç»Ÿè¿”å›çš„é¡ºåºã€‚
* ç‰¹åˆ«æ³¨æ„æ–‡ä»¶ç³»ç»Ÿæ–¹å¼ï¼Œæ–‡ä»¶è¦æ±‚å¿…é¡»æ˜¯UTF-8ç¼–ç ï¼Œå¦åˆ™è¯»å–å‡ºæ¥çš„ä¼šæœ‰ä¹±ç ã€‚

**è¯»å–ç›®å½•**

```scala
val value: RDD[String] = sc.textFile("files")
value.foreach(println)
```

ä¸Šé¢ä»£ç ä¼šè¯»å–filesæ–‡ä»¶ä¸‹çš„æ‰€æœ‰æ–‡ä»¶ã€‚

**è¯»å–å‹ç¼©æ–‡ä»¶**

sparkæ”¯æŒè¯»å–ä½¿ç”¨taræŒ‡ä»¤æ‰“åŒ…çš„å‹ç¼©åŒ…ï¼Œzipçš„å‹ç¼©åŒ…æ— æ³•è¯»å–ã€‚

```scala
val value: RDD[String] = sc.textFile("files/tarfile.tar.gz")
value.foreach(println)
```

å¦‚æœæ˜¯zipçš„å‹ç¼©åŒ…ï¼Œè¯»å–ä¸å‡ºæ¥ï¼Œä¼šå‡ºç°ä¹±ç ï¼Œæ˜¯å¦æœ‰è§£å†³æ–¹æ³•ï¼Œæš‚ä¸çŸ¥æ™“ã€‚

![image-20220913144501263](https://itlab1024-1256529903.cos.ap-beijing.myqcloud.com/202209131445546.png)

**è¯»å–HDFSæ–‡ä»¶**

```scala
val value: RDD[String] = sc.textFile("hdfs://spark-standalone1:9000/wordCount.txt")
value.foreach(println)
```

hdfsç«¯å£ï¼Œæ ¹æ®è‡ªå·±è®¾ç½®çš„é…ç½®ï¼Œæˆ‘é…ç½®çš„æ˜¯9000

```xml
<property>
  <name>fs.defaultFS</name>
  <value>hdfs://spark-standalone1:9000</value>
</property>
```

**wholeTextFilesæ–¹æ³•**

è·Ÿä¸Šé¢ç±»ä¼¼ä¹Ÿæ˜¯è¯»å–æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿæˆ–è€…HDFSï¼Œä¸åŒçš„æ˜¯ä»–ä¼šç”Ÿæˆä¸€ä¸ªkvå…ƒç»„çš„RDD

```scala
val value: RDD[(String, String)] = sc.wholeTextFiles("files")
value.foreach(println)
```

![image-20220913161907868](https://itlab1024-1256529903.cos.ap-beijing.myqcloud.com/202209131619130.png)

## ç®—å­

å‰é¢å·²ç»ç®€å•ä»‹ç»è¿‡ç®—å­ï¼Œè¿™é‡Œæˆ‘è¦ä¸€ä¸ªä¸€ä¸ªå…·ä½“å­¦ä¹ æ¯ä¸€ä¸ªç®—å­çš„å«ä¹‰ä»¥åŠå¦‚ä½•ä½¿ç”¨ã€‚

### map

Sparkä¸­çš„mapå’Œscalaã€javaä¸­çš„mapåŸºæœ¬ç›¸åŒï¼Œé€šè¿‡ä¼ å…¥ä¸€ä¸ªå‡½æ•°ï¼Œå°†å€¼è½¬åŒ–ä¸ºå¦å¤–ä¸€ç§ç»“æœï¼Œå½¢æˆä¸€ç§æ–°çš„RDDï¼Œä¸åŒäºscalaåŸºæœ¬mapä¹‹å¤„åœ¨äºï¼Œsparkä¸­çš„mapæ˜¯å¹¶è¡Œè®¡ç®—çš„ã€‚

```scala
package com.itlab1024.spark.core.operations

import org.apache.spark.rdd.RDD
import org.apache.spark.{SparkConf, SparkContext}

/**
 * 
 *
 * @author itlab
 */
object MapOperator {
  def main(args: Array[String]): Unit = {
    // å®šä¹‰é…ç½®ï¼Œé€šè¿‡é…ç½®å»ºç«‹è¿æ¥
    val conf = new SparkConf().setAppName("åº”ç”¨").setMaster("local[*]")
    val sc = new SparkContext(conf)

    val intRDD = sc.makeRDD(List(1, 2, 3, 4, 5, 6))
    val r = intRDD.map(_ - 1) // å°†RDDä¸­æ¯ä¸ªå€¼å‡1
    r.foreach(println)
    // å…³é—­è¿æ¥
    sc.stop()
  }
}
```

![image-20220913163720751](https://itlab1024-1256529903.cos.ap-beijing.myqcloud.com/202209131637901.png)

mapçš„å¹¶è¡Œè®¡ç®—ï¼Œè¯·æ³¨æ„ä¸‹å›¾ä¸­setMasterå¤„çš„ä¿®æ”¹ã€‚

![image-20220913173723422](https://itlab1024-1256529903.cos.ap-beijing.myqcloud.com/202209131737513.png)

å¯ä»¥çœ‹åˆ°å¹¶éæŒ‰ç…§æ•°æ®é¡ºåºä¸€æ­¥ä¸€æ­¥çš„æ‰§è¡Œï¼Œæ‰§è¡Œé¡ºåºæ˜¯ä¸ç¡®å®šçš„ã€‚

### filter

è¿‡æ»¤å™¨ï¼Œé€šè¿‡æ¡ä»¶ï¼ˆè¿”å›booleanç±»å‹ï¼‰è¿”å›ç»“æœæ˜¯trueçš„å…ƒç´ ç»„æˆçš„æ–°çš„RDD

```scala
package com.itlab1024.spark.core.operations

import org.apache.spark.{SparkConf, SparkContext}

/**
 * 
 *
 * @author itlab
 */
object FilterOperator {
  def main(args: Array[String]): Unit = {
    // å®šä¹‰é…ç½®ï¼Œé€šè¿‡é…ç½®å»ºç«‹è¿æ¥
    val conf = new SparkConf().setAppName("åº”ç”¨").setMaster("local[*]")
    val sc = new SparkContext(conf)

    val intRDD = sc.makeRDD(List(1, 2, 3, 4, 5, 6))
    val r = intRDD.filter(_ > 3) // åªè¦å¤§äº3çš„çš„æ•°æ®
    r.foreach(println)
    // å…³é—­è¿æ¥
    sc.stop()
  }
}
```

![image-20220913164103076](https://itlab1024-1256529903.cos.ap-beijing.myqcloud.com/202209131641202.png)

### flatMap

flatMapåŠŸèƒ½è·Ÿmapç±»ä¼¼ï¼Œä¸åŒä¹‹å¤„åœ¨äºflatï¼Œå¹³é“ºå¼€ï¼Œä»–ä¼šå°†é›†åˆä¸­çš„æ•°æ®å…¨éƒ¨å±•å¼€ã€‚

```scala
package com.itlab1024.spark.core.operations

import org.apache.spark.{SparkConf, SparkContext}

/**
 *
 * @author itlab
 */
object FlatMapOperator {
  def main(args: Array[String]): Unit = {
    // å®šä¹‰é…ç½®ï¼Œé€šè¿‡é…ç½®å»ºç«‹è¿æ¥
    val conf = new SparkConf().setAppName("åº”ç”¨").setMaster("local[*]")
    val sc = new SparkContext(conf)

    val intRDD = sc.makeRDD(Array(List(1, 2), List(3, 4)))
    val r = intRDD.flatMap(data => data) // å°†RDDå±•å¼€ï¼Œæ•°å€¼åŸæ ·è¾“å‡º
    r.foreach(println)
    // å…³é—­è¿æ¥
    sc.stop()
  }
}
```

![image-20220913164930176](https://itlab1024-1256529903.cos.ap-beijing.myqcloud.com/202209131649454.png)

### mapPartition

åŠŸèƒ½ç±»ä¼¼äºmapï¼Œä½†ä¸¤è€…æ˜¯æœ‰åŒºåˆ«çš„ï¼Œè¯·çœ‹å¦‚ä¸‹è¯´æ˜ï¼š

* mapä¸»è¦æ˜¯ç”¨äºæ•°æ®çš„è½¬æ¢ï¼Œæ•°æ®æ•°é‡ä¸ä¼šå˜åŒ–ï¼Œè€ŒmapPartitionçš„å‚æ•°æ˜¯ä¸€ä¸ªè¿­ä»£å™¨ï¼Œå¯ä»¥å®ç°æ•°æ®é‡çš„å‡å°‘æˆ–è€…å¢åŠ ã€‚

* mapæ¯æ¬¡å¤„ç†ä¸€æ¡æ•°æ®ï¼Œè€ŒmapPartitionï¼šæ¯æ¬¡å¤„ç†ä¸€ä¸ªåˆ†åŒºçš„æ•°æ®ï¼Œè¿™ä¸ªåˆ†åŒºçš„æ•°æ®å¤„ç†å®Œåï¼ŒåŸRDDä¸­åˆ†åŒºçš„æ•°æ®æ‰èƒ½é‡Šæ”¾ï¼Œå¯èƒ½å¯¼è‡´OOMã€‚

* å½“å†…å­˜ç©ºé—´è¾ƒå¤§çš„æ—¶å€™å»ºè®®ä½¿ç”¨mapPartitionï¼Œä»¥æé«˜å¤„ç†æ•ˆç‡ã€‚

ç¤ºä¾‹ï¼š

```scala
package com.itlab1024.spark.core.operations

import org.apache.spark.{SparkConf, SparkContext}

object MapPartitionOperator {
  def main(args: Array[String]): Unit = {
    // å®šä¹‰é…ç½®ï¼Œé€šè¿‡é…ç½®å»ºç«‹è¿æ¥
    val conf = new SparkConf().setAppName("åº”ç”¨").setMaster("local[*]")
    val sc = new SparkContext(conf)

    val intRDD = sc.makeRDD(List(1, 2, 3, 4, 5, 6))
    val r = intRDD.mapPartitions(datas => datas)
    r.foreach(println)
    // å…³é—­è¿æ¥
    sc.stop()
  }
}
```

![image-20220913182156452](https://itlab1024-1256529903.cos.ap-beijing.myqcloud.com/202209131821550.png)

### mapPartitionsWithIndex

åŠŸèƒ½ç±»ä¼¼mapPartitionsï¼Œä¸åŒä¹‹å¤„æ˜¯å¯ä»¥è·å¾—åˆ†åŒºçš„ç´¢å¼•ã€‚

![image-20220913184350438](https://itlab1024-1256529903.cos.ap-beijing.myqcloud.com/202209131843562.png)

### glom

glomç®—å­æ˜¯å°†æ¯ä¸ªåˆ†åŒºçš„æ•°æ®ç»„è£…ä¸ºä¸€ä¸ªæ•°ç»„å½¢æˆçš„RDDï¼Œåˆ†åŒºæ•°ç›®ä¿æŒä¸å˜ã€‚

```scala
package com.itlab1024.spark.core.operations

import org.apache.spark.rdd.RDD
import org.apache.spark.{SparkConf, SparkContext}

object GlomOperator {
  def main(args: Array[String]): Unit = {
    // å®šä¹‰é…ç½®ï¼Œé€šè¿‡é…ç½®å»ºç«‹è¿æ¥
    val conf = new SparkConf().setAppName("åº”ç”¨").setMaster("local[*]")
    val sc = new SparkContext(conf)

    val rdd = sc.makeRDD(List(1, 2, 3, 4, 5, 6), 2)
    val rdd1: RDD[Array[Int]] = rdd.glom()
    rdd1.foreach(x=>x.foreach(println)) // è¿™é‡Œä¸ºä»€ä¹ˆæ˜¯ä¸¤æ¬¡å¾ªç¯ï¼Œå°±æ˜¯å› ä¸ºglomä¼šå°†æ¯ä¸ªåˆ†åŒºçš„æ•°æ®ç»„è£…ä¸ºä¸€ä¸ªArrayï¼Œå†å½¢æˆä¸€ä¸ªRDD
    // å…³é—­è¿æ¥
    sc.stop()
  }
}
```

ä¸Šé¢ä»£ç ä¸­ï¼Œåˆå§‹RDDæ˜¯Intç±»å‹çš„ï¼Œä½¿ç”¨glomè¿”å›çš„æ˜¯RDD[Array[Int]]ï¼Œè¿™å°±æ˜¯å°†æ¯ä¸ªåˆ†åŒºçš„æ•°æ®ç»„è£…ä¸ºArrayï¼Œç„¶åå½¢æˆä¸€ä¸ªRDDï¼Œ1,2,3åœ¨ç¬¬ä¸€ä¸ªåˆ†åŒºï¼Œç»„æˆArray[1,2,3]ï¼ŒåŒç†ï¼Œç¬¬äºŒä¸ªåˆ†åŒºçš„ç»„è£…ä¸ºArray[4,5,6]æ•°ç»„ï¼Œè¿™ä¸¤ä¸ªæ•°ç»„å†å½¢æˆä¸€ä¸ªRDDã€‚

![image-20220913201747245](https://itlab1024-1256529903.cos.ap-beijing.myqcloud.com/202209132017427.png)

ä¸Šå›¾ä¸­è¦ç”¨ä¸¤ä¸ªå¾ªç¯ï¼Œç¬¬ä¸€ä¸ªå¾ªç¯å¼å¾ªç¯RDDï¼Œç¬¬äºŒä¸ªæ˜¯å¾ªç¯Arrayã€‚

### groupBy

åˆ†ç»„ï¼Œé€šè¿‡æ¡ä»¶è¿›è¡Œåˆ†ç»„ï¼Œfuncçš„è¿”å›å€¼å°±æ˜¯keyï¼Œåˆ†ç»„æ“ä½œä¸ä¼šå¯¼è‡´åˆ†åŒºæ•°çš„å˜åŒ–ï¼Œä½†æ˜¯ä¼šå¯¼è‡´æ•°æ®è¢«æ‰“ä¹±é‡æ–°ç»„åˆï¼Œè¿™ä¹Ÿå«åšæ´—ç‰Œï¼ˆshuffleï¼‰ã€‚è¿™å¯èƒ½ä¼šå¯¼è‡´æŸä¸ªåˆ†åŒºæ•°æ®é‡æ¿€å¢ï¼Œå‹åŠ›å¢å¤§ï¼Œè¿™ä¹Ÿå«åšæ•°æ®å€¾æ–œã€‚æ•°æ®æ¸…æ´—æ˜¯ä¼˜åŒ–è®¡ç®—çš„ä¸€ä¸ªå…³æ³¨ç‚¹ï¼Œä»¥åå†æ…¢æ…¢å­¦ä¹ ä¼˜åŒ–ã€‚

```scala
package com.itlab1024.spark.core.operations

import org.apache.spark.{SparkConf, SparkContext}

/**
 * 
 *
 * @author itlab
 */
object GroupByOperator {
  def main(args: Array[String]): Unit = {
    // å®šä¹‰é…ç½®ï¼Œé€šè¿‡é…ç½®å»ºç«‹è¿æ¥
    val conf = new SparkConf().setAppName("åº”ç”¨").setMaster("local[*]")
    val sc = new SparkContext(conf)

    val intRDD = sc.makeRDD(List(1, 2, 3, 4, 5, 6), 2)
    val r = intRDD.groupBy(_ % 2 == 0)
    r.foreach(println)
    // å…³é—­è¿æ¥
    sc.stop()
  }
}
```

è¯·çœ‹ä¸Šé¢ä»£ç ä¸­ï¼ŒåŠŸèƒ½æ—¶é—´å…ƒç´ èƒ½å¤Ÿè¢«2æ•´é™¤çš„æ”¾åˆ°ä¸€èµ·ï¼Œä¸èƒ½è¢«æ•´é™¤çš„æ”¾åˆ°ä¸€èµ·ï¼Œåˆ†ç»„çš„keyåªæœ‰trueå’Œfalseã€‚

å‡è®¾RDDä¸­çš„å…ƒç´ éƒ½æ˜¯å¶æ•°ï¼Œé‚£ä¹ˆæ•°æ®groupByç»“æœéƒ½æ˜¯trueçš„å€¼ï¼Œå¯¼è‡´æ‰€æœ‰æ•°æ®é‡æ–°ç»„åˆåˆ°äº†ä¸€èµ·ï¼Œå°±äº§ç”Ÿäº†æ•°æ®å€¾æ–œã€‚

è¿è¡Œç»“æœå¦‚ä¸‹ï¼š

![image-20220914101402949](https://itlab1024-1256529903.cos.ap-beijing.myqcloud.com/202209141014125.png)

### sample

é¡¾åæ€ä¹‰ï¼Œæ ·æœ¬ï¼Œç”¨äºæ•°æ®å–æ ·ï¼Œè¯¥ç®—å­ä¸»è¦åˆ†ä¸ºæŠ½å–æ•°æ®æ˜¯å¦æ”¾å›ï¼Œä¸æ”¾å›çš„æƒ…å†µä½¿ç”¨çš„æ˜¯ä¼¯åŠªåˆ©ç®—æ³•ï¼Œæ”¾å›ä½¿ç”¨çš„æ˜¯æ³Šæ¾åˆ†å¸ƒç®—æ³•ï¼Œå®é™…å¼€å‘ä¸­ä¸»è¦ç”¨äºå‘ç°å€¾æ–œæ•°æ®è§£å†³æ•°æ®å€¾æ–œã€é¢„ä¼°å†…å­˜ç­‰ã€‚

è¯¥ç®—å­æœ‰ä¸‰ä¸ªå‚æ•°ï¼Œ

* withReplacementï¼šä»£è¡¨å…ƒç´ æ˜¯å¦å¯ä»¥å¤šæ¬¡é‡‡æ ·ï¼Œtrueä»£è¡¨å·²ç»æŠ½å–çš„æ•°æ®æ”¾å›ï¼Œfalseä»£è¡¨ä¸æ”¾å›
* fractionï¼šä»£è¡¨æ¯æ¡æ•°æ®æŠ½å–çš„æ¦‚ç‡ï¼Œè¿™é‡Œæ ¹æ®æ˜¯å¦æ”¾å›æƒ…å†µä¸åŒï¼Œå¦‚æœæ˜¯æ”¾å›ï¼Œåˆ™è¡¨ç¤ºé‡å¤æ•°æ®çš„å‡ ç‡ï¼ŒèŒƒå›´å¤§äºç­‰äº 0.è¡¨ç¤ºæ¯ä¸€ä¸ªå…ƒç´ è¢«æœŸæœ›æŠ½å–åˆ°çš„æ¬¡æ•°ï¼Œå¦‚æœæ˜¯ä¸æ”¾å›æŠ½å–ï¼Œè¯¥å‚æ•°ä»£è¡¨æŠ½å–çš„å‡ ç‡ï¼ŒèŒƒå›´åœ¨[0,1]ä¹‹é—´,0ï¼šå…¨ä¸å–ï¼›1ï¼šå…¨å–ï¼›
* seed æŠ½å–æ•°æ®æ—¶ï¼Œéšæœºç®—æ³•çš„ç§å­ï¼Œå¦‚æœä¸ä¼ é€’ï¼Œé»˜è®¤ä½¿ç”¨çš„æ˜¯å½“å‰ç³»ç»Ÿæ—¶é—´ã€‚

ç¤ºä¾‹ï¼š

```scala
package com.itlab1024.spark.core.operations

import org.apache.spark.{SparkConf, SparkContext}

/**
 * 
 *
 * @author itlab
 */
object SampleOperator {
  def main(args: Array[String]): Unit = {
    // å®šä¹‰é…ç½®ï¼Œé€šè¿‡é…ç½®å»ºç«‹è¿æ¥
    val conf = new SparkConf().setAppName("åº”ç”¨").setMaster("local[*]")
    val sc = new SparkContext(conf)

    val intRDD = sc.makeRDD(List(1, 2, 3, 4, 5, 6), 2)
    val r = intRDD.sample(withReplacement = false, 0.5)
    r.foreach(println)
    // å…³é—­è¿æ¥
    sc.stop()
  }
}
```

ä¸Šé¢çš„ä»£ç ä¸­ï¼Œä½¿ç”¨çš„æ˜¯ä¸æ”¾å›æŠ½å–æ–¹å¼ï¼Œé‚£ä¹ˆç¬¬äºŒä¸ªå‚æ•°ï¼ˆä¸Šé¢ä»£ç æ˜¯0.5ï¼‰å°±ä»£è¡¨æ¯ä¸ªå…ƒç´ è¢«æœŸæœ›æŠ½å–çš„æ¬¡æ•°ã€‚

![image-20220914103834179](https://itlab1024-1256529903.cos.ap-beijing.myqcloud.com/202209141038347.png)

![image-20220914103909263](https://itlab1024-1256529903.cos.ap-beijing.myqcloud.com/202209141039407.png)

ä¸Šé¢ä¸¤ä¸ªå›¾æ˜¯æˆ‘æ‰§è¡Œä¸¤æ¬¡çš„ç»“æœï¼Œç»“æœå¹¶ä¸åŒï¼ŒæŠ½æ ·æœ¬èº«å°±æ˜¯ä¸€ä¸ªä¸ç¡®å®šè¿”å›å€¼çš„ä¸œè¥¿ã€‚

### distinct

è¯¥ç®—å­ç”¨äºå¯¹æ•°æ®å»é‡ï¼Œè¯¥ç®—å­æ”¯æŒä¸€ä¸ªå¯é€‰å‚æ•°åˆ†åŒºæ•°é‡numPartitionsï¼Œéœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè¯¥ç®—å­ä¼šå¯¼è‡´æ•°æ®ä¼šé‡æ–°åˆ†åŒºï¼Œä¹Ÿå°±æ˜¯shuffleï¼ˆåº•å±‚ä¼šä½¿ç”¨reduceBykeyç®—å­ï¼‰ã€‚

```scala
package com.itlab1024.spark.core.operations

import org.apache.spark.{SparkConf, SparkContext}

/**
 * 
 *
 * @author itlab
 */
object DistinctOperator {
  def main(args: Array[String]): Unit = {
    // å®šä¹‰é…ç½®ï¼Œé€šè¿‡é…ç½®å»ºç«‹è¿æ¥
    val conf = new SparkConf().setAppName("åº”ç”¨").setMaster("local[*]")
    val sc = new SparkContext(conf)

    val intRDD = sc.makeRDD(List(1, 1, 3, 3, 5, 6), 2)
    val r = intRDD.distinct()
    r.foreach(println)
    // å…³é—­è¿æ¥
    sc.stop()
  }
}
```

ä¸Šé¢ä»£ç ä¸­æˆ‘ä½¿ç”¨çš„æ˜¯æ²¡æœ‰ä¼ é€’åˆ†åŒºæ•°å‚æ•°ï¼Œè¿è¡Œç»“æœå¦‚ä¸‹ï¼š

![image-20220914104155259](https://itlab1024-1256529903.cos.ap-beijing.myqcloud.com/202209141041435.png)

### coalesce

coaleasceæ˜¯åˆå¹¶çš„æ„æ€ï¼Œè¯¥ç®—å­æ˜¯åˆå¹¶åˆ†åŒºï¼Œæ ¹æ®æ•°æ®é‡ç¼©å‡åˆ†åŒºï¼Œç”¨äºå¤§æ•°æ®é›†è¿‡æ»¤åï¼Œæé«˜å°æ•°æ®é›†çš„æ‰§è¡Œæ•ˆç‡

å½“ spark ç¨‹åºä¸­ï¼Œå­˜åœ¨è¿‡å¤šçš„å°ä»»åŠ¡çš„æ—¶å€™ï¼Œå¯ä»¥é€šè¿‡ coalesce æ–¹æ³•ï¼Œæ”¶ç¼©åˆå¹¶åˆ†åŒºï¼Œå‡å°‘

åˆ†åŒºçš„ä¸ªæ•°ï¼Œå‡å°ä»»åŠ¡è°ƒåº¦æˆæœ¬ã€‚

å…·ä½“è¯´æ˜ï¼šå‡è®¾ä¹‹å‰RDDæ˜¯xä¸ªåˆ†åŒºï¼Œä½¿ç”¨coalesceè¦åˆ†ä¸ºyä¸ªåˆ†åŒº

1. x >= yï¼šä¹Ÿå°±æ˜¯è¯´åˆ†åŒºæ•°ç›®è¾ƒå°‘äº†ï¼Œè¿™æ—¶å€™ä¼šå¾—åˆ°ç›®æ ‡åˆ†åŒºæ•°yï¼Œæ˜¯å¦æ´—ç‰Œå–å†³äºç¬¬äºŒä¸ªå‚æ•°shuffleã€‚
2. x < yï¼šç›®æ ‡æ˜¯æƒ³è¦å®ç°åˆ†åŒºæ•°ç›®å¢å¤§ï¼Œè¿™ç§æƒ…å†µä¸‹å¯èƒ½ä¼šå‡ºç°æ— æ³•è¾¾åˆ°ç›®æ ‡çš„æƒ…å†µï¼Œæ¯”å¦‚æ­¤æ—¶è®¾ç½®äº†ç¬¬äºŒä¸ªå‚æ•°shuffle=falseï¼Œåˆ™æœ€ç»ˆå¾—åˆ°çš„åˆ†åŒºæ•°è¿˜æ˜¯xã€‚å•¥ä¹Ÿæ²¡é”™ã€‚ä½†æ˜¯å¦‚æœæ˜¯shuffle=trueåˆ™ä¼šæ‰“ä¹±æ•°æ®é‡æ–°ç»„åˆï¼Œå¾—åˆ°yåˆ†åŒºæ•°ã€‚

```scala
package com.itlab1024.spark.core.operations

import org.apache.spark.{SparkConf, SparkContext}

/**
 *
 * @author itlab
 */
object CoalesceOperator {
  def main(args: Array[String]): Unit = {
    // å®šä¹‰é…ç½®ï¼Œé€šè¿‡é…ç½®å»ºç«‹è¿æ¥
    val conf = new SparkConf().setAppName("åº”ç”¨").setMaster("local[*]")
    val sc = new SparkContext(conf)

    val intRDD = sc.makeRDD(List(1, 1, 3, 3, 5, 6), 4)
    //    val r = intRDD.distinct()
    val r = intRDD.coalesce(2, shuffle = false) // ç¼©å‡åˆ†åŒºæ•°ï¼Œä½†æ˜¯ä¸æ´—ç‰Œ
    r.foreach(println)
    // å…³é—­è¿æ¥
    sc.stop()
  }
}
```

é€šå¸¸ç¼©å‡åˆ†åŒºæ•°çš„æ—¶å€™ä½¿ç”¨è¯¥ç®—å­ã€‚

### repartition

è¯¥ç®—å­æ˜¯coalesceçš„ç¼©å°ç‰ˆåŠŸèƒ½ï¼Œå› ä¸ºä»–å…¶å®å°±æ˜¯è°ƒç”¨äº†coalesce(numPartitions, shuffle = true)ï¼Œå¹¶é™å®šäº†shuffle=trueã€‚è¿™ä¸ªç®—å­ä¸€å®šæœ‰æ´—ç‰Œæ“ä½œã€‚

```scala
package com.itlab1024.spark.core.operations

import org.apache.spark.{SparkConf, SparkContext}

/**
 *
 *
 * @author itlab
 */
object RepartitionOperator {
  def main(args: Array[String]): Unit = {
    // å®šä¹‰é…ç½®ï¼Œé€šè¿‡é…ç½®å»ºç«‹è¿æ¥
    val conf = new SparkConf().setAppName("åº”ç”¨").setMaster("local[*]")
    val sc = new SparkContext(conf)

    val intRDD = sc.makeRDD(List(1, 1, 3, 3, 5, 6), 3)
    intRDD.saveAsTextFile("output1")
    val r = intRDD.repartition(4) // å¢åŠ åˆ†åŒºæ•°ï¼Œæ´—ç‰Œ
    r.saveAsTextFile("output2")
    // å…³é—­è¿æ¥
    sc.stop()
  }
}
```

### sortBy

æ’åºåŠŸèƒ½ï¼Œ æœ‰ä¸‰ä¸ªå‚æ•°ï¼Œç¬¬ä¸€ä¸ªå‚æ•°æ˜¯ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºå¤„ç†æ•°æ®ï¼Œç¬¬äºŒä¸ªæ˜¯æ’åºæ–¹å¼ï¼Œé»˜è®¤æ˜¯trueï¼Œå‡åºï¼Œç¬¬ä¸‰ä¸ªå‚æ•°æ˜¯ç›®æ ‡åˆ†åŒºæ•°ï¼Œ

æ’åºåæ–°äº§ç”Ÿçš„ RDD çš„åˆ†åŒºæ•°ä¸åŸ RDD çš„åˆ†åŒºæ•°ä¸€

è‡´ã€‚è€Œä¸”å­˜åœ¨shuffleã€‚

```scala
package com.itlab1024.spark.core.operations

import org.apache.spark.{SparkConf, SparkContext}

/**
 *
 *
 * @author itlab
 */
object SortByOperator {
  def main(args: Array[String]): Unit = {
    // å®šä¹‰é…ç½®ï¼Œé€šè¿‡é…ç½®å»ºç«‹è¿æ¥
    val conf = new SparkConf().setAppName("åº”ç”¨").setMaster("local[*]")
    val sc = new SparkContext(conf)

    val intRDD = sc.makeRDD(List(1, 6, 3, 2, 5, 4, 100, 43), 3)
    intRDD.saveAsTextFile("output1")
    val r = intRDD.sortBy( _ - 1,true, 4)
    r.saveAsTextFile("output2")
    // å…³é—­è¿æ¥
    sc.stop()
  }
}
```

è¿è¡Œç»“æœè¯´æ˜ï¼š

sortByä¹‹å‰ï¼Œæœ‰ä¸‰ä¸ªåˆ†åŒºï¼Œæ•°æ®åˆ†åˆ«æ˜¯ç¬¬1ä¸ªåˆ†åŒºæ•°æ®æ˜¯[1, 6]ï¼Œç¬¬äºŒä¸ªæ˜¯[3, 2, 5], ç¬¬ä¸‰ä¸ªæ˜¯[4, 100, 43]

sourtByæ—¶å€™ï¼Œæœ‰å››é˜¿å“¥åˆ†åŒºï¼Œç¬¬ä¸€ä¸ª[1, 2]ï¼Œ ç¬¬äºŒä¸ª[3, 4]ï¼Œ ç¬¬ä¸‰ä¸ª[5, 6]ï¼Œ ç¬¬å››ä¸ª[43, 100]

ä¸Šé¢çš„è¿è¡Œç»“æœä¹Ÿè¯´æ˜äº†æ´—ç‰Œçš„å­˜åœ¨ã€‚

### intersection

äº¤é›†ï¼Œè¦æ±‚ä¸¤ä¸ªé›†åˆç±»å‹å¿…é¡»ä¸€è‡´ã€‚

```scala
package com.itlab1024.spark.core.operations

import org.apache.spark.{SparkConf, SparkContext}

/**
 *
 *
 * @author itlab
 */
object IntersectionOperator {
  def main(args: Array[String]): Unit = {
    // å®šä¹‰é…ç½®ï¼Œé€šè¿‡é…ç½®å»ºç«‹è¿æ¥
    val conf = new SparkConf().setAppName("åº”ç”¨").setMaster("local[*]")
    val sc = new SparkContext(conf)

    val rdd1 = sc.makeRDD(1 to 5, 2) // [1, 2, 3, 4, 5]
    val rdd2 = sc.makeRDD(5 to 9, 3) // [5, 6, 7, 8, 9]
    val rdd3 = rdd1.intersection(rdd2) // äº¤é›†æ˜¯5
    rdd3.foreach(println)// [5]
    // å…³é—­è¿æ¥
    sc.stop()
  }
}
```

### union

å¹¶é›†ï¼Œä¼šè¿”å›ä¸¤ä¸ªé›†åˆçš„ç»„åˆï¼Œä¸ä¼šå»é‡ã€‚ä¹Ÿè¦æ±‚ä¸¤ä¸ªRDDç±»å‹ä¸€è‡´ã€‚

```scala
package com.itlab1024.spark.core.operations

import org.apache.spark.{SparkConf, SparkContext}

/**
 *
 *
 * @author itlab
 */
object UnionOperator {
  def main(args: Array[String]): Unit = {
    // å®šä¹‰é…ç½®ï¼Œé€šè¿‡é…ç½®å»ºç«‹è¿æ¥
    val conf = new SparkConf().setAppName("åº”ç”¨").setMaster("local[*]")
    val sc = new SparkContext(conf)

    val rdd1 = sc.makeRDD(1 to 5, 2) // [1, 2, 3, 4, 5]
    val rdd2 = sc.makeRDD(5 to 9, 3) // [5, 6, 7, 8, 9]
    val rdd3 = rdd1.union(rdd2) // å¹¶é›†æ˜¯[1, 2, 3, 4, 5, 5, 6, 7, 8, 9]
    rdd3.foreach(println)// [5]
    // å…³é—­è¿æ¥
    sc.stop()
  }
}
```

### subtract

æ±‚ä¸¤ä¸ªRDDçš„å·®é›†ï¼Œè·å–RDD1ä¸­å…ƒç´ ä¸å­˜åœ¨äºRDD2ä¸­çš„æ‰€æœ‰å…ƒç´ ç»„æˆæ–°çš„RDD

```scala
package com.itlab1024.spark.core.operations

import org.apache.spark.{SparkConf, SparkContext}

/**
 *
 *
 * @author itlab
 */
object SubtractOperator {
  def main(args: Array[String]): Unit = {
    // å®šä¹‰é…ç½®ï¼Œé€šè¿‡é…ç½®å»ºç«‹è¿æ¥
    val conf = new SparkConf().setAppName("åº”ç”¨").setMaster("local[*]")
    val sc = new SparkContext(conf)

    val rdd1 = sc.makeRDD(1 to 5, 2) // [1, 2, 3, 4, 5]
    val rdd2 = sc.makeRDD(5 to 9, 3) // [5, 6, 7, 8, 9]
    val rdd3 = rdd1.subtract(rdd2)
    rdd3.foreach(println)// [1, 2, 3, 4]
    // å…³é—­è¿æ¥
    sc.stop()
  }
}
```

### zip

å‹ç¼©åŠŸèƒ½ï¼Œå°†ä¸¤ä¸ªRDDå‹ç¼©ä¸ºä¸€ä¸ªï¼Œè¯¥ç®—å­ä¸è¦æ±‚ä¸¤ä¸ªRDDçš„ç±»å‹ä¸€è‡´ï¼Œä½†æ˜¯è¦æ±‚ä¸¤ä¸ªRDDçš„åˆ†åŒºæ•°ä¸€æ ·ï¼Œå¹¶ä¸”è¦æ±‚ç›¸åŒæ•°ç›®çš„åˆ†åŒºå…ƒç´ ã€‚

```scala
package com.itlab1024.spark.core.operations

import org.apache.spark.{SparkConf, SparkContext}

/**
 *
 *
 * @author itlab
 */
object ZipOperator {
  def main(args: Array[String]): Unit = {
    // å®šä¹‰é…ç½®ï¼Œé€šè¿‡é…ç½®å»ºç«‹è¿æ¥
    val conf = new SparkConf().setAppName("åº”ç”¨").setMaster("local[*]")
    val sc = new SparkContext(conf)

    val rdd1 = sc.makeRDD(1 to 5, 3) // [1, 2, 3, 4, 5]
    val rdd2 = sc.makeRDD(List("a", "b", "c", "d", "e"), 3) // [5, 6, 7, 8, 9]
    val rdd3 = rdd1.zip(rdd2)
    rdd3.foreach(println)// [1, 2, 3, 4]
    // å…³é—­è¿æ¥
    sc.stop()
  }
}
```

![image-20220914144508321](https://itlab1024-1256529903.cos.ap-beijing.myqcloud.com/202209141445426.png)

### partitionBy

é€šè¿‡åˆ†åŒºkeyé‡æ–°åˆ†åŒºï¼Œ è¯¥ç®—å­æ˜¯PairRDDFunctionsä¸‹çš„æ–¹æ³•ï¼Œä¹Ÿå°±æ˜¯è¯´ä»–æ˜¯PairRDDçš„ç‰¹æœ‰æ–¹æ³•ã€‚éœ€è¦ä¸€ä¸ªHashPartitioneråˆ†åŒºå™¨

```scala
package com.itlab1024.spark.core.operations

import org.apache.spark.rdd.RDD
import org.apache.spark.{SparkConf, SparkContext}

/**
 *
 *
 * @author itlab
 */
object PartitionByOperator {
  def main(args: Array[String]): Unit = {
    // å®šä¹‰é…ç½®ï¼Œé€šè¿‡é…ç½®å»ºç«‹è¿æ¥
    val conf = new SparkConf().setAppName("åº”ç”¨").setMaster("local[*]")
    val sc = new SparkContext(conf)

    val rdd: RDD[(Int, String)] =
      sc.makeRDD(List((1, "a"), (2, "b"), (3, "c")), 3)
    rdd.saveAsTextFile("output1")
    import org.apache.spark.HashPartitioner
    val r: RDD[(Int, String)] =
      rdd.partitionBy(new HashPartitioner(2))
    r.saveAsTextFile("output2")
    // å…³é—­è¿æ¥
    sc.stop()
  }
}
```

è¯·æ³¨æ„RDDçš„ç±»å‹ã€‚

### reduceByKey

å¯¹RDDç›¸åŒçš„keyå®ç°valueçš„èšåˆï¼Œä¹Ÿæ˜¯è¦æ±‚RDDç±»å‹æ˜¯é”®å€¼å¯¹ã€‚

```scala
package com.itlab1024.spark.core.operations

import org.apache.spark.rdd.RDD
import org.apache.spark.{SparkConf, SparkContext}

/**
 *
 *
 * @author itlab
 */
object ReduceByKeyOperator {
  def main(args: Array[String]): Unit = {
    // å®šä¹‰é…ç½®ï¼Œé€šè¿‡é…ç½®å»ºç«‹è¿æ¥
    val conf = new SparkConf().setAppName("åº”ç”¨").setMaster("local[*]")
    val sc = new SparkContext(conf)

    val rdd = sc.makeRDD(List((1, "a"), (1, "b"), (3, "c")))
    val r = rdd.reduceByKey(_ + _)
    r.foreach(println) // [(1,ab),(3,c)]
    // å…³é—­è¿æ¥
    sc.stop()
  }
}
```

### groupByKey

åªåˆ†ç»„ä¸èšåˆï¼Œè¿™ä¸åŒäºä¸Šé¢è¯´çš„reduceByKeyï¼ŒreduceByKeyé»˜è®¤æœ‰åˆ†ç»„æ“ä½œï¼Œç„¶åæ‰§è¡Œèšåˆï¼ŒgroupByKeyæ²¡æœ‰èšåˆçš„å«ä¹‰ï¼Œå¦‚æœæƒ³å®ç°reduceByKeyä¸€æ ·çš„åŠŸèƒ½ï¼Œè¿˜éœ€è¦æ­é…èšåˆç®—å­ã€‚ç›¸åŒçš„æ˜¯ä¸¤è€…éƒ½æœ‰shuffleã€‚

```scala
package com.itlab1024.spark.core.operations

import org.apache.spark.{SparkConf, SparkContext}

/**
 *
 *
 * @author itlab
 */
object GroupByKeyOperator {
  def main(args: Array[String]): Unit = {
    // å®šä¹‰é…ç½®ï¼Œé€šè¿‡é…ç½®å»ºç«‹è¿æ¥
    val conf = new SparkConf().setAppName("åº”ç”¨").setMaster("local[*]")
    val sc = new SparkContext(conf)

    val rdd = sc.makeRDD(List((1, "a"), (1, "b"), (3, "c")))
    val r = rdd.groupBy(_._1) // é€šè¿‡å…ƒç»„çš„ç¬¬ä¸€ä¸ªå…ƒç»„åˆ†ç»„
    r.foreach(println) // [(3,CompactBuffer((3,c))) (1, CompactBuffer((1, a), (1, b)))]
    // å…³é—­è¿æ¥
    sc.stop()
  }
}
```

ä¸¤è€…å…·ä½“åŒºåˆ«ï¼ˆæ‘˜è‡ªå°šç¡…è°·çš„ä¸€æ®µè¯ï¼‰ï¼š

**ä»** **shuffle** **çš„è§’åº¦**ï¼šreduceByKey å’Œ groupByKey éƒ½å­˜åœ¨ shuffle çš„æ“ä½œï¼Œä½†æ˜¯ reduceByKey

å¯ä»¥åœ¨ shuffle å‰å¯¹åˆ†åŒºå†…ç›¸åŒ key çš„æ•°æ®è¿›è¡Œé¢„èšåˆï¼ˆcombineï¼‰åŠŸèƒ½ï¼Œè¿™æ ·ä¼šå‡å°‘è½ç›˜çš„

æ•°æ®é‡ï¼Œè€Œ groupByKey åªæ˜¯è¿›è¡Œåˆ†ç»„ï¼Œä¸å­˜åœ¨æ•°æ®é‡å‡å°‘çš„é—®é¢˜ï¼ŒreduceByKey æ€§èƒ½æ¯”è¾ƒ

é«˜ã€‚

**ä»åŠŸèƒ½çš„è§’åº¦**ï¼šreduceByKey å…¶å®åŒ…å«åˆ†ç»„å’Œèšåˆçš„åŠŸèƒ½ã€‚GroupByKey åªèƒ½åˆ†ç»„ï¼Œä¸èƒ½èš

åˆï¼Œæ‰€ä»¥åœ¨åˆ†ç»„èšåˆçš„åœºåˆä¸‹ï¼Œæ¨èä½¿ç”¨ reduceByKeyï¼Œå¦‚æœä»…ä»…æ˜¯åˆ†ç»„è€Œä¸éœ€è¦èšåˆã€‚é‚£

ä¹ˆè¿˜æ˜¯åªèƒ½ä½¿ç”¨ groupByKey

### aggregateByKey

é€šè¿‡keyèšåˆï¼Œä¸åŒäºreduceByKeyçš„æ˜¯ï¼Œè¯¥ç®—å­å°†æ•°æ®æ ¹æ®ä¸åŒçš„è§„åˆ™è¿›è¡Œåˆ†åŒºå†…è®¡ç®—å’Œåˆ†åŒºé—´è®¡ç®—ï¼Œå¯ä»¥åˆ†åˆ«æŒ‡å®šåˆ†åŒºå†…å’Œåˆ†åŒºé—´èšåˆçš„æ–¹æ³•ï¼ˆå¯ä»¥ä¸åŒï¼‰ã€‚

è¯¥ç®—å­ä½¿ç”¨äº†å‡½æ•°çš„æŸ¯é‡Œè¯ï¼Œæœ‰ä¸¤ä¸ªå‚æ•°åˆ—è¡¨ï¼Œç¬¬ä¸€ä¸ªå‚æ•°åˆ—è¡¨æœ‰ä¸€ä¸ªå‚æ•°ï¼Œä»£è¡¨åˆå§‹å€¼ï¼Œç¬¬äºŒä¸ªå‚æ•°åˆ—è¡¨æœ‰ä¸¤ä¸ªå‚æ•°ï¼Œå‚æ•°ä¸€ä»£è¡¨å‡åŒºé—´çš„è®¡ç®—å‡½æ•°ï¼Œå‚æ•°2ä»£è¡¨åˆ†åŒºå†…çš„å‡½æ•°ã€‚

æ¯”å¦‚æˆ‘å¯ä»¥é€‰æ‹©å°†åˆ†åŒºå†…çš„è®¡ç®—ä½¿ç”¨è·å–æœ€å¤§å€¼ï¼Œåˆ†åŒºé—´çš„èšåˆä½¿ç”¨ç›¸åŠ ï¼Œçœ‹å¦‚ä¸‹ç¤ºä¾‹ã€‚

```scala
package com.itlab1024.spark.core.operations

import org.apache.spark.{SparkConf, SparkContext}

/**
 *
 *
 * @author itlab
 */
object AggregateByKeyOperator {
  def main(args: Array[String]): Unit = {
    // å®šä¹‰é…ç½®ï¼Œé€šè¿‡é…ç½®å»ºç«‹è¿æ¥
    val conf = new SparkConf().setAppName("åº”ç”¨").setMaster("local[*]")
    val sc = new SparkContext(conf)

    // ç¬¬ä¸€ä¸ªåˆ†åŒºæ•°æ®æ˜¯("a", 1), ("a", 2), ("c", 3)
    // ç¬¬äºŒä¸ªåˆ†åŒºæ•°æ®æ˜¯("b", 4), ("c", 5), ("c", 6)
    val rdd = sc.makeRDD(List(
      ("a", 1), ("a", 2), ("c", 3),
      ("b", 4), ("c", 5), ("c", 6)
    ), 2)
    // åˆå§‹é›¶å€¼æ˜¯=0
    val r = rdd.aggregateByKey(0)((x, y) => {
      math.max(x, y)
    }, _ + _)
    r.foreach(println)
    // å…³é—­è¿æ¥
    sc.stop()
  }
}
```

åˆ†æä¸‹ï¼Œæ•´æ•°ä»£ç ä¸­æ³¨é‡Šæ‰€å†™çš„é‚£æ ·ï¼Œåˆå§‹æ•°æ®è¢«åˆ†ä¸ºä¸¤ä¸ªåˆ†åŒºï¼Œ// ç¬¬ä¸€ä¸ªåˆ†åŒºæ•°æ®æ˜¯("a", 1), ("a", 2), ("c", 3)ï¼Œ// ç¬¬äºŒä¸ªåˆ†åŒºæ•°æ®æ˜¯("b", 4), ("c", 5), ("c", 6)ï¼Œé›¶å€¼=0ï¼Œåˆ†åŒºå†…çš„ç®—æ³•æ˜¯ç›¸åŒçš„keyè·å–æœ€å¤§çš„ï¼Œæ‰€ä»¥ç¬¬ä¸€ä¸ªåˆ†åŒºå°±å˜ä¸ºäº†("a", 2), ("c", 3)ï¼Œç¬¬äºŒä¸ªåˆ†åŒºå°±æ˜¯("b", 4), ("c", 6)ï¼Œé‚£ä¹ˆæœ€ç»ˆçš„ç»“æœå°±åº”è¯¥æ˜¯ (a,2) (b,4) (c,9)ã€‚

çœ‹ä¸‹ä»£ç è¿è¡Œæˆªå›¾

![image-20220916174337942](https://itlab1024-1256529903.cos.ap-beijing.myqcloud.com/202209161743473.png)

ç¡®å®æ˜¯è¿™æ ·ã€‚
