> æœ¬äººè¿›å…¥å¤§æ•°æ®é¢†åŸŸä»…ä»…ä¸€å¹´ï¼Œä¹‹å‰ä»æœªæ¥è§¦è¿‡å¤§æ•°æ®é¢†åŸŸï¼Œå¯¹äºSparkæ›´æ˜¯é—»æ‰€æœªé—»ï¼Œç›®å‰å…¬å¸ä¸»è¦åšIOTé¢†åŸŸã€‚å¤§æ•°æ®æŠ€æœ¯æ˜¯å¿…é¡»è¦æŒæ¡çš„ï¼Œ
> è€Œä¸”é¡¹ç›®ä¸­ä¹Ÿä½¿ç”¨åˆ°äº†Sparkï¼Œæ‰€ä»¥å­¦ä¹ ä¸‹ï¼ŒSparkæ˜¯ä½¿ç”¨Scalaå¼€å‘çš„ã€‚æœ€å¥½ä¹ŸæŒæ¡ä¸‹Scalaè¯­è¨€ã€‚
> æˆ‘å­¦ä¹ Sparkä¸»è¦æœ‰é€šè¿‡å®˜ç½‘ã€å°šç¡…è°·(ä¸å¾—ä¸è¯´å°šç¡…è°·çœŸçš„æ˜¯ä¸šç•Œè‰¯å¿ƒï¼Œä¸æ˜¯æ‰“å¹¿å‘Šå•ŠğŸ˜„)ï¼Œå¦å¤–å°±æ˜¯ä¸æ˜ç™½çš„æ—¶å€™é—®é—®åº¦å¨˜ã€‚
# ä»€ä¹ˆæ˜¯Spark?
![](https://itlab1024-1256529903.cos.ap-beijing.myqcloud.com/202208312008157.png)
Spark æ˜¯ApacheåŸºé‡‘ä¼šå¼€å‘çš„ç”¨äºå¤§æ•°æ®å¤„ç†çš„ç»Ÿä¸€åˆ†æå¼•æ“ã€‚å®ƒæä¾› Javaã€Scalaã€Python å’Œ R è¯­è¨€ä¸­çš„é«˜çº§ APIï¼Œ
è¿˜æœ‰æœºå™¨å­¦ä¹ ã€å›¾å½¢å¤„ç†ç­‰ï¼ˆè¿™ä¸¤ä¸ªæˆ‘ç›®å‰ç”¨ä¸åˆ°ï¼Œå…ˆä¸å­¦äº†ã€‚ï¼‰
# ä¸‹è½½å®‰è£…
ç›®å‰æœ€æ–°ç‰ˆæœ¬æ˜¯3.3.0ã€‚æˆ‘å°±æ˜¯ç”¨è¿™ä¸ªç‰ˆæœ¬è¿›è¡Œå­¦ä¹ äº†ã€‚
æ‰“å¼€[Sparkä¸‹è½½åœ°å€](https://spark.apache.org/downloads.html)
![](https://itlab1024-1256529903.cos.ap-beijing.myqcloud.com/202208312006177.png)
ä¸‹è½½å®Œæˆåï¼Œè§£å‹ï¼Œå¦‚ä¸‹å›¾ï¼š
![](https://itlab1024-1256529903.cos.ap-beijing.myqcloud.com/202208312037740.png)
# äº¤äº’Shellå’Œè¿è¡Œç¤ºä¾‹
sparkçš„binç›®å½•ä¸‹æä¾›äº†ä¸€ä¸ªäº¤äº’shellï¼Œå¹¶ä¸”ä»–æä¾›äº†å¾ˆå¤šä¾‹å­ï¼Œæ¥ä¸‹æ¥æˆ‘å°±è¿è¡Œspark-shellè„šæœ¬ï¼Œæ‰“å¼€äº¤äº’å‘½ä»¤è¡Œ
```shell
âœ  spark-3.3.0-bin-hadoop3 bin/spark-shell 
22/08/31 20:12:00 WARN Utils: Your hostname, ITshiyanshideMacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 10.112.82.59 instead (on interface en0)
22/08/31 20:12:00 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
22/08/31 20:12:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Spark context Web UI available at http://10.112.82.59:4040
Spark context available as 'sc' (master = local[*], app id = local-1661947928082).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.3.0
      /_/
         
Using Scala version 2.12.15 (Java HotSpot(TM) 64-Bit Server VM, Java 17.0.3.1)
Type in expressions to have them evaluated.
Type :help for more information.

scala> 
```
å¯ä»¥çœ‹åˆ°ä¸Šé¢å°±æ‰“å¼€äº†ä¸€ä¸ªshellã€‚åœ¨è¿™é‡Œå°±å¯ä»¥è¾“å‡ºscalaä»£ç ï¼Œæ¯”å¦‚ï¼š
```scala
scala> val i = 1
i: Int = 1

scala> val j = 1
j: Int = 1
scala> println(i + j)
2
```
æ²¡æœ‰é—®é¢˜ã€‚sparkä¸­å·²ç»ä¸ºæˆ‘ä»¬åˆå§‹åŒ–äº†sparkContextå’ŒsparkSessionï¼Œå˜é‡åˆ†åˆ«å¯¹åº”çš„æ˜¯scå’Œsparkã€‚
ä¹Ÿå¯ä»¥æŒ‡å®šå‚æ•°å¯åŠ¨shellï¼Œæ¯”å¦‚
```shell
âœ  spark-3.3.0-bin-hadoop3 bin/spark-shell --master 'local[2]' 
22/08/31 20:21:06 WARN Utils: Your hostname, ITshiyanshideMacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 10.112.82.59 instead (on interface en0)
22/08/31 20:21:06 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
22/08/31 20:21:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Spark context Web UI available at http://10.112.82.59:4040
Spark context available as 'sc' (master = local[2], app id = local-1661948473413).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.3.0
      /_/
         
Using Scala version 2.12.15 (Java HotSpot(TM) 64-Bit Server VM, Java 17.0.3.1)
Type in expressions to have them evaluated.
Type :help for more information.

scala> 
```
localä»£è¡¨æœ¬åœ°æ¨¡å¼ï¼Œsparkè¿˜æœ‰standaloneæ¨¡å¼ï¼Œyarnæ¨¡å¼ç­‰ç­‰ï¼Œæ…¢æ…¢å­¦ã€‚[2]è¿™ä¸ª2ä»£è¡¨çº¿ç¨‹æ•°ï¼Œå…·ä½“å«ä¹‰ä»¥åæ…¢æ…¢å­¦ä¹ ã€‚

æ¥ä¸‹æ¥æˆ‘ä½¿ç”¨run-exampleæ¥è¿è¡Œä¸€ä¸ªå†…ç½®çš„PIè®¡ç®—çš„ä»»åŠ¡ã€‚
```shell
âœ  spark-3.3.0-bin-hadoop3 bin/run-example SparkPi 10
22/08/31 20:18:25 WARN Utils: Your hostname, ITshiyanshideMacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 10.112.82.59 instead (on interface en0)
22/08/31 20:18:25 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
22/08/31 20:18:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
22/08/31 20:18:25 INFO SparkContext: Running Spark version 3.3.0
22/08/31 20:18:25 INFO ResourceUtils: ==============================================================
22/08/31 20:18:25 INFO ResourceUtils: No custom resources configured for spark.driver.
22/08/31 20:18:25 INFO ResourceUtils: ==============================================================
22/08/31 20:18:25 INFO SparkContext: Submitted application: Spark Pi
22/08/31 20:18:25 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
22/08/31 20:18:26 INFO ResourceProfile: Limiting resource is cpu
22/08/31 20:18:26 INFO ResourceProfileManager: Added ResourceProfile id: 0
22/08/31 20:18:26 INFO SecurityManager: Changing view acls to: itlab
22/08/31 20:18:26 INFO SecurityManager: Changing modify acls to: itlab
22/08/31 20:18:26 INFO SecurityManager: Changing view acls groups to: 
22/08/31 20:18:26 INFO SecurityManager: Changing modify acls groups to: 
22/08/31 20:18:26 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(itlab); groups with view permissions: Set(); users  with modify permissions: Set(itlab); groups with modify permissions: Set()
22/08/31 20:18:26 INFO Utils: Successfully started service 'sparkDriver' on port 65391.
22/08/31 20:18:26 INFO SparkEnv: Registering MapOutputTracker
22/08/31 20:18:26 INFO SparkEnv: Registering BlockManagerMaster
22/08/31 20:18:26 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
22/08/31 20:18:26 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
22/08/31 20:18:26 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
22/08/31 20:18:26 INFO DiskBlockManager: Created local directory at /private/var/folders/b7/yhnw9hws0ng2w1_khl8nr2t40000gn/T/blockmgr-006f1115-02e9-4883-96c4-7deb4c9c0f2c
22/08/31 20:18:26 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
22/08/31 20:18:26 INFO SparkEnv: Registering OutputCommitCoordinator
22/08/31 20:18:27 INFO Utils: Successfully started service 'SparkUI' on port 4040.
22/08/31 20:18:27 INFO SparkContext: Added JAR file:///Users/itlab/dev-tools/spark-3.3.0-bin-hadoop3/examples/jars/scopt_2.12-3.7.1.jar at spark://10.112.82.59:65391/jars/scopt_2.12-3.7.1.jar with timestamp 1661948305908
22/08/31 20:18:27 INFO SparkContext: Added JAR file:///Users/itlab/dev-tools/spark-3.3.0-bin-hadoop3/examples/jars/spark-examples_2.12-3.3.0.jar at spark://10.112.82.59:65391/jars/spark-examples_2.12-3.3.0.jar with timestamp 1661948305908
22/08/31 20:18:27 INFO SparkContext: The JAR file:/Users/itlab/dev-tools/spark-3.3.0-bin-hadoop3/examples/jars/spark-examples_2.12-3.3.0.jar at spark://10.112.82.59:65391/jars/spark-examples_2.12-3.3.0.jar has been added already. Overwriting of added jar is not supported in the current version.
22/08/31 20:18:27 INFO Executor: Starting executor ID driver on host 10.112.82.59
22/08/31 20:18:27 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
22/08/31 20:18:27 INFO Executor: Fetching spark://10.112.82.59:65391/jars/scopt_2.12-3.7.1.jar with timestamp 1661948305908
22/08/31 20:18:27 INFO TransportClientFactory: Successfully created connection to /10.112.82.59:65391 after 39 ms (0 ms spent in bootstraps)
22/08/31 20:18:27 INFO Utils: Fetching spark://10.112.82.59:65391/jars/scopt_2.12-3.7.1.jar to /private/var/folders/b7/yhnw9hws0ng2w1_khl8nr2t40000gn/T/spark-12cd6bb6-84c2-4145-90bc-0aa083c2b6c4/userFiles-ce4cfeb0-70e4-42b7-9f37-e4143760ff2a/fetchFileTemp5878280159627562337.tmp
22/08/31 20:18:27 INFO Executor: Adding file:/private/var/folders/b7/yhnw9hws0ng2w1_khl8nr2t40000gn/T/spark-12cd6bb6-84c2-4145-90bc-0aa083c2b6c4/userFiles-ce4cfeb0-70e4-42b7-9f37-e4143760ff2a/scopt_2.12-3.7.1.jar to class loader
22/08/31 20:18:27 INFO Executor: Fetching spark://10.112.82.59:65391/jars/spark-examples_2.12-3.3.0.jar with timestamp 1661948305908
22/08/31 20:18:27 INFO Utils: Fetching spark://10.112.82.59:65391/jars/spark-examples_2.12-3.3.0.jar to /private/var/folders/b7/yhnw9hws0ng2w1_khl8nr2t40000gn/T/spark-12cd6bb6-84c2-4145-90bc-0aa083c2b6c4/userFiles-ce4cfeb0-70e4-42b7-9f37-e4143760ff2a/fetchFileTemp2553748991312385625.tmp
22/08/31 20:18:27 INFO Executor: Adding file:/private/var/folders/b7/yhnw9hws0ng2w1_khl8nr2t40000gn/T/spark-12cd6bb6-84c2-4145-90bc-0aa083c2b6c4/userFiles-ce4cfeb0-70e4-42b7-9f37-e4143760ff2a/spark-examples_2.12-3.3.0.jar to class loader
22/08/31 20:18:27 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 65394.
22/08/31 20:18:27 INFO NettyBlockTransferService: Server created on 10.112.82.59:65394
22/08/31 20:18:27 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
22/08/31 20:18:27 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.112.82.59, 65394, None)
22/08/31 20:18:27 INFO BlockManagerMasterEndpoint: Registering block manager 10.112.82.59:65394 with 434.4 MiB RAM, BlockManagerId(driver, 10.112.82.59, 65394, None)
22/08/31 20:18:27 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.112.82.59, 65394, None)
22/08/31 20:18:27 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.112.82.59, 65394, None)
22/08/31 20:18:28 INFO SparkContext: Starting job: reduce at SparkPi.scala:38
22/08/31 20:18:28 INFO DAGScheduler: Got job 0 (reduce at SparkPi.scala:38) with 10 output partitions
22/08/31 20:18:28 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkPi.scala:38)
22/08/31 20:18:28 INFO DAGScheduler: Parents of final stage: List()
22/08/31 20:18:28 INFO DAGScheduler: Missing parents: List()
22/08/31 20:18:28 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at map at SparkPi.scala:34), which has no missing parents
22/08/31 20:18:28 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 4.0 KiB, free 434.4 MiB)
22/08/31 20:18:28 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 2.3 KiB, free 434.4 MiB)
22/08/31 20:18:28 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.112.82.59:65394 (size: 2.3 KiB, free: 434.4 MiB)
22/08/31 20:18:28 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1513
22/08/31 20:18:28 INFO DAGScheduler: Submitting 10 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at SparkPi.scala:34) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))
22/08/31 20:18:28 INFO TaskSchedulerImpl: Adding task set 0.0 with 10 tasks resource profile 0
22/08/31 20:18:28 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.112.82.59, executor driver, partition 0, PROCESS_LOCAL, 4578 bytes) taskResourceAssignments Map()
22/08/31 20:18:28 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (10.112.82.59, executor driver, partition 1, PROCESS_LOCAL, 4578 bytes) taskResourceAssignments Map()
22/08/31 20:18:28 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2) (10.112.82.59, executor driver, partition 2, PROCESS_LOCAL, 4578 bytes) taskResourceAssignments Map()
22/08/31 20:18:28 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3) (10.112.82.59, executor driver, partition 3, PROCESS_LOCAL, 4578 bytes) taskResourceAssignments Map()
22/08/31 20:18:28 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
22/08/31 20:18:28 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
22/08/31 20:18:28 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
22/08/31 20:18:28 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
22/08/31 20:18:28 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 1008 bytes result sent to driver
22/08/31 20:18:28 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 1008 bytes result sent to driver
22/08/31 20:18:28 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1008 bytes result sent to driver
22/08/31 20:18:28 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1008 bytes result sent to driver
22/08/31 20:18:28 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4) (10.112.82.59, executor driver, partition 4, PROCESS_LOCAL, 4578 bytes) taskResourceAssignments Map()
22/08/31 20:18:28 INFO Executor: Running task 4.0 in stage 0.0 (TID 4)
22/08/31 20:18:28 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5) (10.112.82.59, executor driver, partition 5, PROCESS_LOCAL, 4578 bytes) taskResourceAssignments Map()
22/08/31 20:18:28 INFO Executor: Running task 5.0 in stage 0.0 (TID 5)
22/08/31 20:18:28 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6) (10.112.82.59, executor driver, partition 6, PROCESS_LOCAL, 4578 bytes) taskResourceAssignments Map()
22/08/31 20:18:28 INFO Executor: Running task 6.0 in stage 0.0 (TID 6)
22/08/31 20:18:28 INFO TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7) (10.112.82.59, executor driver, partition 7, PROCESS_LOCAL, 4578 bytes) taskResourceAssignments Map()
22/08/31 20:18:28 INFO Executor: Running task 7.0 in stage 0.0 (TID 7)
22/08/31 20:18:28 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 334 ms on 10.112.82.59 (executor driver) (1/10)
22/08/31 20:18:28 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 323 ms on 10.112.82.59 (executor driver) (2/10)
22/08/31 20:18:28 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 332 ms on 10.112.82.59 (executor driver) (3/10)
22/08/31 20:18:28 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 338 ms on 10.112.82.59 (executor driver) (4/10)
22/08/31 20:18:28 INFO Executor: Finished task 5.0 in stage 0.0 (TID 5). 965 bytes result sent to driver
22/08/31 20:18:28 INFO TaskSetManager: Starting task 8.0 in stage 0.0 (TID 8) (10.112.82.59, executor driver, partition 8, PROCESS_LOCAL, 4578 bytes) taskResourceAssignments Map()
22/08/31 20:18:28 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 139 ms on 10.112.82.59 (executor driver) (5/10)
22/08/31 20:18:28 INFO Executor: Running task 8.0 in stage 0.0 (TID 8)
22/08/31 20:18:29 INFO Executor: Finished task 4.0 in stage 0.0 (TID 4). 965 bytes result sent to driver
22/08/31 20:18:29 INFO TaskSetManager: Starting task 9.0 in stage 0.0 (TID 9) (10.112.82.59, executor driver, partition 9, PROCESS_LOCAL, 4578 bytes) taskResourceAssignments Map()
22/08/31 20:18:29 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 184 ms on 10.112.82.59 (executor driver) (6/10)
22/08/31 20:18:29 INFO Executor: Finished task 7.0 in stage 0.0 (TID 7). 965 bytes result sent to driver
22/08/31 20:18:29 INFO Executor: Running task 9.0 in stage 0.0 (TID 9)
22/08/31 20:18:29 INFO TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 180 ms on 10.112.82.59 (executor driver) (7/10)
22/08/31 20:18:29 INFO Executor: Finished task 6.0 in stage 0.0 (TID 6). 965 bytes result sent to driver
22/08/31 20:18:29 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 190 ms on 10.112.82.59 (executor driver) (8/10)
22/08/31 20:18:29 INFO Executor: Finished task 8.0 in stage 0.0 (TID 8). 965 bytes result sent to driver
22/08/31 20:18:29 INFO TaskSetManager: Finished task 8.0 in stage 0.0 (TID 8) in 77 ms on 10.112.82.59 (executor driver) (9/10)
22/08/31 20:18:29 INFO Executor: Finished task 9.0 in stage 0.0 (TID 9). 965 bytes result sent to driver
22/08/31 20:18:29 INFO TaskSetManager: Finished task 9.0 in stage 0.0 (TID 9) in 69 ms on 10.112.82.59 (executor driver) (10/10)
22/08/31 20:18:29 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
22/08/31 20:18:29 INFO DAGScheduler: ResultStage 0 (reduce at SparkPi.scala:38) finished in 0.808 s
22/08/31 20:18:29 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
22/08/31 20:18:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
22/08/31 20:18:29 INFO DAGScheduler: Job 0 finished: reduce at SparkPi.scala:38, took 0.882169 s
Pi is roughly 3.14032714032714
22/08/31 20:18:29 INFO SparkUI: Stopped Spark web UI at http://10.112.82.59:4040
22/08/31 20:18:29 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
22/08/31 20:18:29 INFO MemoryStore: MemoryStore cleared
22/08/31 20:18:29 INFO BlockManager: BlockManager stopped
22/08/31 20:18:29 INFO BlockManagerMaster: BlockManagerMaster stopped
22/08/31 20:18:29 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
22/08/31 20:18:29 INFO SparkContext: Successfully stopped SparkContext
22/08/31 20:18:29 INFO ShutdownHookManager: Shutdown hook called
22/08/31 20:18:29 INFO ShutdownHookManager: Deleting directory /private/var/folders/b7/yhnw9hws0ng2w1_khl8nr2t40000gn/T/spark-12cd6bb6-84c2-4145-90bc-0aa083c2b6c4
22/08/31 20:18:29 INFO ShutdownHookManager: Deleting directory /private/var/folders/b7/yhnw9hws0ng2w1_khl8nr2t40000gn/T/spark-250b1c99-4798-4faf-b534-6ecdac448570
```
å¯ä»¥çœ‹åˆ°æ—¥å¿—ä¸­ï¼šPi is roughly 3.14032714032714ï¼Œè¿™å°±æ˜¯Piçš„è¿ç®—ç»“æœã€‚
# å¿«é€Ÿä¸Šæ‰‹
Sparkå¼€å‘å¯ä»¥ä½¿ç”¨VS Codeå·¥å…·ï¼Œæˆ–è€…IDEAã€‚æˆ‘ä½¿ç”¨çš„æ˜¯IDEAã€‚è®°ä¸‹æ¥ç®€å•ä»‹ç»ä¸‹ä½¿ç”¨IDEAåˆ›å»ºä¸€ä¸ªé¡¹ç›®ã€‚
å› ä¸ºæˆ‘è¦ä½¿ç”¨scalaè¯­è¨€å»å†™sparkï¼ˆä¹Ÿå¯ä»¥ä½¿ç”¨Javaã€Pythonã€Rç­‰è¯­è¨€ï¼‰ï¼Œé¡¹ç›®ç®¡ç†å¯ä»¥ä½¿ç”¨Mavenã€Gradleã€SBT
SBTæ˜¯scalaé¡¹ç›®çš„åŒ…ç®¡ç†å·¥å…·ï¼Œæˆ‘è¿™å°±ä½¿ç”¨SBTã€‚
## åˆ›å»ºé¡¹ç›®
![](https://itlab1024-1256529903.cos.ap-beijing.myqcloud.com/202209011344345.png)
## å¼•å…¥sparkä¾èµ–
è¿™é‡Œæˆ‘å…ˆæ˜ å…¥spring-coreåŒ…ã€‚å…¶ä»–çš„åŒ…ä»¥åç”¨åˆ°å†å¼•å…¥ï¼Œbuild.sbtæ˜¯sbtçš„ä¾èµ–é…ç½®ï¼Œç±»ä¼¼mavençš„pomã€‚
```sbt
ThisBuild / version := "0.1.0-SNAPSHOT"

ThisBuild / scalaVersion := "2.12.16"

lazy val root = (project in file("."))
  .settings(
    name := "Spark-Tutorial"
  )
libraryDependencies += "org.apache.spark" % "spark-core_2.12" % "3.3.0"
```
